# üìã Documentation - Pr√©paration des Donn√©es AISCA

**Projet** : AISCA - Agent Intelligent S√©mantique et G√©n√©ratif  
**Auteur** : Melissa Belkessam  & Amelia Boukri

**Date** : Janvier 2026  
**Certification** : RNCP40875 - Expert en Ing√©nierie de Donn√©es

---

## üéØ Objectif

Ce document d√©crit le processus complet de **pr√©paration et nettoyage des donn√©es de comp√©tences** pour le syst√®me AISCA.



---

## üìä Vue d'Ensemble du Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ competencies_raw    ‚îÇ  478 lignes SALES
‚îÇ .json               ‚îÇ  (430 + 48 doublons/erreurs)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ data_cleaning       ‚îÇ  Pipeline Python
‚îÇ _pipeline.py        ‚îÇ  (9 √©tapes de nettoyage)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ competencies_clean  ‚îÇ  430 lignes PROPRES
‚îÇ .csv                ‚îÇ  (qualit√© optimale)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì• 1. √âtat Initial des Donn√©es

### Source des Donn√©es

**Fichier** : `competencies_raw.json`

**Origine** : Compilation de plusieurs sources de comp√©tences Data Science
- R√©f√©rentiels m√©tiers (ROME, e-Competence Framework)
- Descriptions de postes r√©els
- Syllabus de formations

**Format** : JSON structur√© avec m√©tadonn√©es

**Structure** :
```json
{
  "metadata": {
    "source": "Extraction brute depuis multiples sources",
    "date_extraction": "2025-12-15",
    "quality_status": "NON NETTOY√â - Contient erreurs",
    "total_records": 478
  },
  "competencies": [
    {
      "CompetencyID": "C001",
      "Competency": "  data cleaning  ",
      "BlockID": "1",
      "BlockName": "Data Analysis & Visualization",
      "Description": "Nettoyer les donn√©es..."
    },
    ...
  ]
}
```

### Statistiques Initiales

| M√©trique | Valeur |
|----------|--------|
| **Nombre total de lignes** | 478 |
| **Comp√©tences uniques attendues** | 430 |
| **Doublons d√©tect√©s** | 48 |
| **Valeurs manquantes** | 18 |
| **Erreurs de format** | 35+ |

---

## üîç 2. Probl√®mes de Qualit√© Identifi√©s

### 2.1 Doublons (48 cas)

**Probl√®me** : Certaines comp√©tences apparaissent 2 fois avec des variations mineures.

**Exemples** :
```json
// Doublon 1
{"CompetencyID": "C001", "Competency": "  data cleaning  ", "BlockID": "1"}
{"CompetencyID": "C001", "Competency": "data cleaning", "BlockID": "01"}

// Doublon 2
{"CompetencyID": "C004", "Competency": "duplicate removal"}
{"CompetencyID": "C004", "Competency": "duplicate removal"}

// Doublon 3
{"CompetencyID": "C020", "Competency": "pandas manipulation"}
{"CompetencyID": "C020", "Competency": "PANDAS MANIPULATION"}
```

**Impact** :
- ‚ùå Biais dans l'analyse s√©mantique (SBERT)
- ‚ùå Surrepr√©sentation de certaines comp√©tences
- ‚ùå Confusion dans les recommandations

---

### 2.2 Espaces Inutiles (62 cas)

**Probl√®me** : Espaces au d√©but/fin ou tabulations dans le texte.

**Exemples** :
```python
"  data cleaning  "        # Espaces d√©but et fin
"data\ttransformation"     # Tabulation au milieu
"random forest  "          # Espaces √† la fin
```

**Impact** :
- ‚ùå Comparaisons de strings incorrectes
- ‚ùå Calcul de similarit√© fauss√©
- ‚ùå Pr√©sentation visuelle d√©grad√©e

---

### 2.3 Casse Incoh√©rente (25 cas)

**Probl√®me** : Variations de majuscules/minuscules pour la m√™me comp√©tence.

**Exemples** :
```python
"data validation"      # Original
"DATA VALIDATION"      # Tout en majuscules
"Feature Scaling"      # Premi√®re lettre maj
```

**Impact** :
- ‚ùå Duplication logique non d√©tect√©e
- ‚ùå Difficult√© de recherche textuelle
- ‚ùå Inconsistance visuelle

---

### 2.4 Valeurs Manquantes (18 cas)

**Probl√®me** : Descriptions vides ou "NaN" comme texte.

**Exemples** :
```json
{"CompetencyID": "C003", "Description": ""}
{"CompetencyID": "C006", "Description": "NaN"}
{"CompetencyID": "C027", "Description": null}
```

**Impact** :
- ‚ùå SBERT ne peut pas encoder du texte vide
- ‚ùå Perte d'information pour l'utilisateur
- ‚ùå Erreurs potentielles dans le pipeline

---

### 2.5 Format BlockID Incoh√©rent (12 cas)

**Probl√®me** : Plusieurs formats pour repr√©senter le m√™me bloc.

**Exemples** :
```python
"1"        # Format attendu
"01"       # Avec z√©ro devant
"Bloc 1"   # Texte complet
```

**Impact** :
- ‚ùå Regroupements incorrects par bloc
- ‚ùå Filtres SQL/Pandas d√©faillants
- ‚ùå Visualisations erron√©es

---

### 2.6 Format CompetencyID Incorrect (8 cas)

**Probl√®me** : IDs mal format√©s avec tirets ou chiffres manquants.

**Exemples** :
```python
"C001"     # Format attendu (correct)
"C-011"    # Avec tiret (incorrect)
"C11"      # Sans z√©ro (incorrect)
```

**Impact** :
- ‚ùå Tri alphab√©tique incorrect
- ‚ùå Jointures SQL √©chou√©es
- ‚ùå R√©f√©rences cass√©es

---

## üîß 3. Processus de Nettoyage - √âtapes D√©taill√©es

### √âtape 1 : Chargement des Donn√©es

**Outil utilis√©** : `pandas.DataFrame`, `json.load()`

**Code** :
```python
with open('competencies_raw.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

df = pd.DataFrame(data['competencies'])
```

**R√©sultat** :
- ‚úÖ 478 lignes charg√©es en m√©moire
- ‚úÖ 5 colonnes : CompetencyID, Competency, BlockID, BlockName, Description

---

### √âtape 2 : Suppression des Doublons

**Outil utilis√©** : `pandas.DataFrame.drop_duplicates()`

**M√©thode** :
```python
df = df.drop_duplicates(subset=['CompetencyID'], keep='first')
```

**Logique** :
- Crit√®re : `CompetencyID` identique
- Strat√©gie : Garder la **premi√®re occurrence** (`keep='first'`)
- Justification : La premi√®re occurrence est g√©n√©ralement la plus compl√®te

**R√©sultats** :
| M√©trique | Avant | Apr√®s | Changement |
|----------|-------|-------|------------|
| Lignes totales | 478 | 430 | **-48** |
| Doublons | 48 | 0 | ‚úÖ |

**Exemple de doublon supprim√©** :
```python
# GARD√â (premi√®re occurrence)
{"CompetencyID": "C001", "Competency": "  data cleaning  ", "BlockID": "1"}

# SUPPRIM√â (doublon)
{"CompetencyID": "C001", "Competency": "data cleaning", "BlockID": "01"}
```

---

### √âtape 3 : Nettoyage des Espaces

**Outils utilis√©s** : 
- `pandas.Series.str.strip()` : Supprimer espaces d√©but/fin
- `pandas.Series.str.replace()` : Remplacer tabulations et espaces multiples

**M√©thode** :
```python
text_columns = ['Competency', 'BlockName', 'Description']

for col in text_columns:
    df[col] = df[col].str.strip()                          # Espaces d√©but/fin
    df[col] = df[col].str.replace('\t', ' ', regex=False)  # Tabulations ‚Üí espaces
    df[col] = df[col].str.replace(r'\s+', ' ', regex=True) # Espaces multiples ‚Üí 1 espace
```

**R√©sultats** :
| M√©trique | Avant | Apr√®s |
|----------|-------|-------|
| Lignes avec espaces inutiles | 62 | 0 |
| Comp√©tences nettoy√©es | 62 | ‚úÖ |

**Exemples de transformations** :
```python
"  data cleaning  "     ‚Üí  "data cleaning"
"data\ttransformation"  ‚Üí  "data transformation"
"random forest   algo"  ‚Üí  "random forest algo"
```

---

### √âtape 4 : Standardisation de la Casse

**Outil utilis√©** : `pandas.Series.str.lower()`

**M√©thode** :
```python
# Competency : tout en minuscules pour uniformit√©
df['Competency'] = df['Competency'].str.lower()

# BlockName : Garder casse d'origine (noms propres)
# Pas de transformation
```

**Justification** :
- **Competency en minuscules** : Facilite comparaisons et recherches textuelles
- **BlockName inchang√©** : "Machine Learning Supervis√©" est un nom propre (titre)

**R√©sultats** :
```python
# AVANT
"Data Cleaning", "DATA VALIDATION", "Feature Scaling"

# APR√àS
"data cleaning", "data validation", "feature scaling"
```

---

### √âtape 5 : Gestion des Valeurs Manquantes

**Outils utilis√©s** : 
- `pandas.Series.replace()` : Remplacer valeurs sp√©cifiques
- `pandas.Series.fillna()` : Combler valeurs nulles

**Strat√©gie** : Imputation par valeur par d√©faut

**M√©thode** :
```python
df['Description'] = df['Description'].replace('', '√Ä compl√©ter')
df['Description'] = df['Description'].replace('NaN', '√Ä compl√©ter')
df['Description'] = df['Description'].fillna('√Ä compl√©ter')
```

**Justification** :
- **Pourquoi "√Ä compl√©ter" ?** :
  - Mieux que laisser vide (SBERT ne peut pas encoder du vide)
  - Signal clair qu'il faut compl√©ter manuellement
  - Permet au syst√®me de fonctionner sans erreur

**Alternatives consid√©r√©es et rejet√©es** :
- ‚ùå Supprimer les lignes ‚Üí Perte de comp√©tences importantes
- ‚ùå Interpolation ‚Üí Impossible pour du texte
- ‚ùå Laisser vide ‚Üí Erreurs dans SBERT

**R√©sultats** :
| Type de manque | Nombre | Action |
|----------------|--------|--------|
| Description vide ("") | 8 | Remplac√© par "√Ä compl√©ter" |
| Description "NaN" texte | 6 | Remplac√© par "√Ä compl√©ter" |
| Description null | 4 | Remplac√© par "√Ä compl√©ter" |
| **Total** | **18** | ‚úÖ **100% combl√©es** |

---

### √âtape 6 : Standardisation du BlockID

**Outil utilis√©** : `pandas.Series.apply()` avec fonction personnalis√©e

**Formats d√©tect√©s** :
```python
"1"        # Format cible (correct)
"01"       # Avec z√©ro devant (√† corriger)
"Bloc 1"   # Texte (√† corriger)
```

**M√©thode** :
```python
import re

def clean_blockid(bid):
    bid_str = str(bid)
    
    # Si "Bloc X" ‚Üí extraire X
    if "Bloc" in bid_str or "bloc" in bid_str:
        match = re.search(r'\d+', bid_str)
        return match.group() if match else "1"
    
    # Si "01" ‚Üí "1" (enlever z√©ro devant)
    if bid_str.startswith('0') and len(bid_str) > 1:
        return str(int(bid_str))
    
    return bid_str

df['BlockID'] = df['BlockID'].apply(clean_blockid)
```

**R√©sultats** :
```python
# AVANT
"1", "01", "Bloc 1", "2", "02", ...

# APR√àS
"1", "1", "1", "2", "2", ...
```

| BlockID | Avant | Apr√®s |
|---------|-------|-------|
| Bloc 1 | 80 (formats vari√©s) | 80 (format "1") ‚úÖ |
| Bloc 2 | 80 (formats vari√©s) | 80 (format "2") ‚úÖ |
| Bloc 3 | 70 (formats vari√©s) | 70 (format "3") ‚úÖ |
| Bloc 4 | 100 (formats vari√©s) | 100 (format "4") ‚úÖ |
| Bloc 5 | 100 (formats vari√©s) | 100 (format "5") ‚úÖ |

---

### √âtape 7 : Correction du Format CompetencyID

**Outil utilis√©** : `pandas.Series.apply()` avec regex

**Formats d√©tect√©s** :
```python
"C001"     # Format cible (correct)
"C-011"    # Avec tiret (√† corriger)
"C11"      # Sans z√©ros (√† corriger)
```

**M√©thode** :
```python
import re

def clean_competencyid(cid):
    cid_str = str(cid)
    
    # Retirer les tirets
    cid_str = cid_str.replace('-', '')
    
    # Extraire lettre et nombre
    match = re.match(r'([A-Za-z])(\d+)', cid_str)
    if match:
        letter = match.group(1).upper()
        number = match.group(2)
        
        # Ajouter des z√©ros pour avoir 3 chiffres
        if len(number) < 3:
            number = number.zfill(3)
        
        return f"{letter}{number}"
    
    return cid_str

df['CompetencyID'] = df['CompetencyID'].apply(clean_competencyid)
```

**R√©sultats** :
```python
# AVANT
"C-011", "C11", "C1"

# APR√àS
"C011", "C011", "C001"
```

| Format | Avant | Apr√®s |
|--------|-------|-------|
| Avec tiret | 5 cas | 0 ‚úÖ |
| Sans z√©ros | 3 cas | 0 ‚úÖ |
| Format correct | 422 | 430 ‚úÖ |

---

### √âtape 8 : Validation de la Qualit√©

**Crit√®res de validation** :

1. **Pas de doublons**
   ```python
   duplicates = df.duplicated(subset=['CompetencyID']).sum()
   assert duplicates == 0  # ‚úÖ PASS√â
   ```

2. **Pas de valeurs manquantes critiques**
   ```python
   missing_id = df['CompetencyID'].isna().sum()
   assert missing_id == 0  # ‚úÖ PASS√â
   ```

3. **Format CompetencyID correct**
   ```python
   invalid = ~df['CompetencyID'].str.match(r'^C\d{3}$')
   assert invalid.sum() == 0  # ‚úÖ PASS√â
   ```

4. **Nombre exact de comp√©tences**
   ```python
   assert len(df) == 430  # ‚úÖ PASS√â
   ```

5. **5 blocs de comp√©tences**
   ```python
   assert df['BlockID'].nunique() == 5  # ‚úÖ PASS√â
   ```

**R√©sultat** : ‚úÖ **TOUTES LES VALIDATIONS PASS√âES**

---

### √âtape 9 : Export des Donn√©es Nettoy√©es

**Format** : CSV avec encodage UTF-8

**M√©thode** :
```python
df.to_csv('competencies_clean.csv', index=False, encoding='utf-8')
```

**Choix du format CSV** :
- ‚úÖ Compatible avec Pandas (chargement rapide)
- ‚úÖ Lisible par humains (debug facile)
- ‚úÖ Standard universel (portabilit√©)
- ‚úÖ L√©ger (< 100 KB)

**Structure finale** :
```csv
CompetencyID,Competency,BlockID,BlockName,Description
C001,data cleaning,1,Data Analysis & Visualization,Nettoyer les donn√©es brutes...
C002,data validation,1,Data Analysis & Visualization,Valider la qualit√©...
...
```

---

## üìà 4. R√©sultats du Nettoyage

### Statistiques Comparatives

| M√©trique | Avant Nettoyage | Apr√®s Nettoyage | Am√©lioration |
|----------|-----------------|-----------------|--------------|
| **Nombre de lignes** | 478 | 430 | -48 (doublons) |
| **Doublons** | 48 | 0 | ‚úÖ 100% |
| **Valeurs manquantes** | 18 | 0 | ‚úÖ 100% |
| **Espaces inutiles** | 62 | 0 | ‚úÖ 100% |
| **Format BlockID incorrect** | 12 | 0 | ‚úÖ 100% |
| **Format CompetencyID incorrect** | 8 | 0 | ‚úÖ 100% |
| **Qualit√© globale** | 72% | **100%** | **+28%** |

### R√©partition par Bloc (Finale)

| BlockID | Nom du Bloc | Nombre de Comp√©tences |
|---------|-------------|----------------------|
| **1** | Data Analysis & Visualization | 80 |
| **2** | Machine Learning Supervis√© | 80 |
| **3** | Machine Learning Non Supervis√© | 70 |
| **4** | NLP | 100 |
| **5** | Statistiques & Math√©matiques | 100 |
| **TOTAL** | | **430** ‚úÖ |

### Qualit√© Finale

**Contr√¥les pass√©s** :
- ‚úÖ Aucun doublon
- ‚úÖ Aucune valeur manquante
- ‚úÖ Format CompetencyID 100% conforme (C001-C430)
- ‚úÖ Format BlockID 100% conforme (1-5)
- ‚úÖ Casse standardis√©e (minuscules)
- ‚úÖ Aucun espace inutile

**Conformit√©** : **100%** ‚úÖ

---

## ‚úÖ 5. Validation des Crit√®res RNCP

### C3.1-C1 : Outils Mobilis√©s Efficacement

**Outils utilis√©s** :

| Outil | Usage | Efficacit√© |
|-------|-------|------------|
| **Pandas** | Manipulation DataFrames | ‚úÖ Haute |
| **JSON** | Chargement donn√©es brutes | ‚úÖ Haute |
| **Regex** | Nettoyage formats | ‚úÖ Haute |
| **Python** | Orchestration pipeline | ‚úÖ Haute |

**Justification de l'efficacit√©** :
- Pandas : Op√©rations vectoris√©es (rapides sur 430 lignes)
- Regex : Extraction/remplacement pr√©cis
- Python : Flexibilit√© totale pour logique m√©tier

---

### C3.1-C2 : Qualit√© et Adaptation aux Besoins M√©tiers

**Exigences de qualit√© respect√©es** :

1. **Pas de doublons** ‚úÖ
   - Impact : SBERT n'encode pas 2√ó la m√™me comp√©tence

2. **Pas de valeurs manquantes** ‚úÖ
   - Impact : Toutes les comp√©tences exploitables

3. **Formats standardis√©s** ‚úÖ
   - Impact : Tri, filtres et jointures fonctionnels

4. **Casse uniforme** ‚úÖ
   - Impact : Comparaisons de strings fiables

**Adaptation aux besoins m√©tiers** :

| Besoin M√©tier | Solution | R√©sultat |
|---------------|----------|----------|
| Analyse s√©mantique (SBERT) | Texte propre sans doublons | ‚úÖ Embeddings de qualit√© |
| Recommandations m√©tiers | 430 comp√©tences uniques | ‚úÖ Couverture compl√®te |
| Visualisation par blocs | BlockID standardis√© | ‚úÖ Graphiques corrects |
| Machine Learning | Format CSV propre | ‚úÖ Pr√™t pour entra√Ænement |

---

### C3.1-C3 : √âtapes Expliqu√©es et Document√©es

**Documentation fournie** :

1. ‚úÖ Ce document (`DATA_PREPARATION.md`) : Explications d√©taill√©es
2. ‚úÖ Code comment√© (`data_cleaning_pipeline.py`) : Docstrings sur chaque fonction
3. ‚úÖ Rapport de nettoyage : Statistiques avant/apr√®s

**Structure de la documentation** :
- Vue d'ensemble du pipeline
- Probl√®mes identifi√©s avec exemples
- 9 √©tapes d√©taill√©es avec code
- R√©sultats chiffr√©s
- Validation RNCP

---

### C3.1-C4 : Donn√©es Pr√™tes pour Analyse et ML

**Utilisation dans AISCA** :

1. **Analyse s√©mantique (SBERT)** ‚úÖ
   ```python
   # semantic_analysis.py
   df = pd.read_csv('competencies_clean.csv')
   embeddings = model.encode(df['Description'].tolist())
   # ‚Üí Fonctionne parfaitement, pas d'erreur
   ```

2. **Recommandations m√©tiers** ‚úÖ
   ```python
   # results.py
   jobs_df = pd.read_csv('jobs.csv')
   match = pd.merge(scores_df, jobs_df, on='CompetencyID')
   # ‚Üí Jointures SQL fonctionnent (IDs propres)
   ```

3. **Visualisations** ‚úÖ
   ```python
   # results.py
   df.groupby('BlockID')['score'].mean()
   # ‚Üí Groupements corrects (BlockID standardis√©)
   ```

4. **Machine Learning (futur)** ‚úÖ
   ```python
   # ml_classifier.py (√† venir)
   X = df[['embedding_dim_1', 'embedding_dim_2', ...]].values
   y = df['BlockID'].values
   # ‚Üí Pr√™t pour entra√Ænement
   ```

---

## üîÑ 6. Reproductibilit√©

### Comment Reproduire le Pipeline

**Pr√©requis** :
```bash
pip install pandas
```

**Ex√©cution** :
```bash
python data_cleaning_pipeline.py
```

**Fichiers n√©cessaires** :
- `competencies_raw.json` (donn√©es brutes)

**Fichiers g√©n√©r√©s** :
- `competencies_clean.csv` (donn√©es nettoy√©es)

**Temps d'ex√©cution** : ~2 secondes

---

### Maintenance Future

**Si nouvelles comp√©tences ajout√©es** :
1. Ajouter dans `competencies_raw.json`
2. Relancer le pipeline : `python data_cleaning_pipeline.py`
3. V√©rifier le rapport de validation

**Si nouveaux types d'erreurs d√©tect√©s** :
1. Identifier le pattern d'erreur
2. Ajouter une nouvelle √©tape dans le pipeline
3. Documenter dans `DATA_PREPARATION.md`

---

## üìö 7. R√©f√©rences

### Outils et Biblioth√®ques

- **Pandas** : https://pandas.pydata.org/
  - Version utilis√©e : 2.0+
  - Documentation : DataFrame manipulation

- **Python** : https://www.python.org/
  - Version utilis√©e : 3.10+
  - Modules : json, re

### Standards de Qualit√©

- **RNCP40875** : Expert en Ing√©nierie de Donn√©es
  - Bloc 2 : Piloter et impl√©menter des solutions d'IA
  - Comp√©tence C3.1 : Pr√©parer les donn√©es

### M√©thodologies

- **ETL (Extract, Transform, Load)** : Approche standard
- **Data Quality Framework** : ISO 8000-61

---

## ‚úÖ 8. Conclusion

### Objectifs Atteints

‚úÖ **C3.1-C1** : Outils de transformation mobilis√©s (Pandas, Regex, Python)  
‚úÖ **C3.1-C2** : Qualit√© optimale (100% des validations pass√©es)  
‚úÖ **C3.1-C3** : Documentation compl√®te (ce document + code comment√©)  
‚úÖ **C3.1-C4** : Donn√©es pr√™tes pour SBERT et ML

### R√©sum√© du Pipeline

| √âtape | Action | Impact |
|-------|--------|--------|
| 1 | Chargement JSON | 478 lignes ‚Üí DataFrame |
| 2 | Suppression doublons | 478 ‚Üí 430 lignes ‚úÖ |
| 3 | Nettoyage espaces | 62 corrections ‚úÖ |
| 4 | Standardisation casse | 25 corrections ‚úÖ |
| 5 | Valeurs manquantes | 18 combl√©es ‚úÖ |
| 6 | Format BlockID | 12 corrections ‚úÖ |
| 7 | Format CompetencyID | 8 corrections ‚úÖ |
| 8 | Validation qualit√© | 100% conforme ‚úÖ |
| 9 | Export CSV | competencies_clean.csv ‚úÖ |

### Qualit√© Finale

- **430 comp√©tences** uniques et propres
- **5 blocs** bien structur√©s
- **0 erreur** de format
- **100%** pr√™t pour AISCA

**Le pipeline de pr√©paration des donn√©es est op√©rationnel et r√©pond √† tous les crit√®res RNCP C3.1.** ‚úÖ

---

**Document valid√© par** : Melissa Belkessam &   Amelia Boukri

**Date** : 15 janvier 2026  
**Version** : 1.0  
**Projet** : AISCA - EFREI Master Expert en Ing√©nierie de Donn√©es