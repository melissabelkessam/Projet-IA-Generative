"""
AISCA - Module Gemini avec Cache Automatique
G√©n√©ration du Plan de Progression et Bio Professionnelle
VERSION ROBUSTE avec d√©tection automatique des mod√®les
"""

import google.generativeai as genai
import json
import os
from datetime import datetime
from typing import Dict, Optional
from dotenv import load_dotenv

# Charger la cl√© API depuis .env
load_dotenv()
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

if not GEMINI_API_KEY:
    raise ValueError("‚ùå GEMINI_API_KEY manquante ! Cr√©ez un fichier .env avec votre cl√©.")

# Configurer Gemini
genai.configure(api_key=GEMINI_API_KEY)

# Chemin du fichier cache
CACHE_FILE = 'data/gemini_cache.json'

# Variable globale pour stocker le mod√®le d√©tect√©
DETECTED_MODEL = None


def detect_available_model():
    """
    D√©tecter automatiquement le premier mod√®le Gemini disponible
    
    Returns:
        str: Nom du mod√®le disponible
    """
    global DETECTED_MODEL
    
    if DETECTED_MODEL:
        return DETECTED_MODEL
    
    print("\nüîç D√©tection des mod√®les Gemini disponibles...")
    
    try:
        # Liste des mod√®les √† essayer dans l'ordre de pr√©f√©rence
        preferred_models = [
            'gemini-2.0-flash-exp',
            'gemini-1.5-flash',
            'gemini-1.5-flash-latest',
            'gemini-1.5-pro',
            'gemini-1.5-pro-latest',
            'gemini-pro',
            'gemini-1.0-pro'
        ]
        
        # Lister tous les mod√®les disponibles
        available_models = []
        for model in genai.list_models():
            if 'generateContent' in model.supported_generation_methods:
                model_name = model.name.replace('models/', '')
                available_models.append(model_name)
        
        print(f"üìã Mod√®les d√©tect√©s : {available_models}")
        
        # Chercher le premier mod√®le pr√©f√©r√© disponible
        for preferred in preferred_models:
            if preferred in available_models:
                DETECTED_MODEL = preferred
                print(f"‚úÖ Mod√®le s√©lectionn√© : {DETECTED_MODEL}")
                return DETECTED_MODEL
        
        # Si aucun mod√®le pr√©f√©r√©, prendre le premier disponible
        if available_models:
            DETECTED_MODEL = available_models[0]
            print(f"‚ö†Ô∏è Utilisation du mod√®le par d√©faut : {DETECTED_MODEL}")
            return DETECTED_MODEL
        
        # Aucun mod√®le disponible
        raise ValueError("‚ùå Aucun mod√®le Gemini disponible avec cette cl√© API")
    
    except Exception as e:
        print(f"‚ùå Erreur d√©tection mod√®le : {e}")
        # Fallback : essayer gemini-pro en dernier recours
        DETECTED_MODEL = 'gemini-pro'
        print(f"üîÑ Tentative avec mod√®le par d√©faut : {DETECTED_MODEL}")
        return DETECTED_MODEL


def load_cache() -> Dict:
    """Charger le cache depuis le fichier JSON"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture cache : {e}")
            return {}
    return {}


def save_cache(cache: Dict):
    """Sauvegarder le cache dans le fichier JSON"""
    try:
        os.makedirs('data', exist_ok=True)
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde cache : {e}")


def generate_cache_key(request_type: str, profile_data: Dict) -> str:
    """
    G√©n√©rer une cl√© unique pour le cache
    
    Args:
        request_type: 'progression' ou 'bio'
        profile_data: Donn√©es du profil utilisateur
        
    Returns:
        Cl√© unique bas√©e sur les scores et m√©tier
    """
    try:
        # Cr√©er une signature unique du profil
        scores = profile_data.get('block_scores', {})
        jobs = profile_data.get('recommended_jobs', [])
        job = jobs[0].get('job_title', 'unknown') if jobs else 'unknown'
        
        # Arrondir les scores pour regrouper les profils similaires
        signature = f"{request_type}_"
        for bloc in ['bloc1', 'bloc2', 'bloc3', 'bloc4', 'bloc5']:
            score = scores.get(bloc, {}).get('score', 0)
            # Arrondir √† 0.1 pr√®s pour cr√©er des groupes
            rounded = round(score * 10) / 10
            signature += f"{bloc}_{rounded}_"
        
        signature += job.replace(' ', '_')
        
        return signature
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur g√©n√©ration cl√© cache : {e}")
        return f"{request_type}_default"


def generate_progression_plan(analysis_results: Dict) -> str:
    """
    G√©n√©rer un plan de progression personnalis√© avec CACHE
    UN SEUL APPEL API par profil unique
    
    Args:
        analysis_results: R√©sultats de l'analyse SBERT
        
    Returns:
        Plan de progression (str)
    """
    print("\nüîç G√©n√©ration du Plan de Progression...")
    
    # Charger le cache
    cache = load_cache()
    
    # G√©n√©rer la cl√© de cache
    cache_key = generate_cache_key('progression', analysis_results)
    
    # V√©rifier si d√©j√† en cache
    if cache_key in cache:
        print("‚úÖ Plan trouv√© dans le cache ! (Aucun appel API)")
        return cache[cache_key]['response']
    
    print("üåê Appel API Gemini (nouveau profil)...")
    
    try:
        # D√©tecter le mod√®le disponible
        model_name = detect_available_model()
        
        # Identifier les blocs FAIBLES (score < 0.5)
        weak_blocks = []
        block_scores = analysis_results.get('block_scores', {})
        
        for bloc_key, bloc_data in block_scores.items():
            score = bloc_data.get('score', 0)
            if score < 0.5:
                weak_blocks.append({
                    'bloc': bloc_key,
                    'score': score,
                    'sbert_score': bloc_data.get('sbert_score', 0),
                    'likert_score': bloc_data.get('likert_score', 0)
                })
        
        # Trier par score croissant (les plus faibles en premier)
        weak_blocks = sorted(weak_blocks, key=lambda x: x['score'])[:3]
        
        # Construire le prompt
        prompt = f"""Tu es un expert en formation Data Science et IA.

Analyse ce profil de comp√©tences et cr√©e un plan de progression personnalis√©.

**Blocs de comp√©tences √† am√©liorer (scores faibles) :**
"""
        
        bloc_names = {
            'bloc1': 'Data Analysis & Visualization',
            'bloc2': 'Machine Learning Supervis√©',
            'bloc3': 'Machine Learning Non Supervis√©',
            'bloc4': 'NLP (Natural Language Processing)',
            'bloc5': 'Statistiques & Math√©matiques'
        }
        
        for weak in weak_blocks:
            bloc_name = bloc_names.get(weak['bloc'], weak['bloc'])
            prompt += f"\n- **{bloc_name}** : Score actuel {weak['score']:.1%}"
        
        recommended_jobs = analysis_results.get('recommended_jobs', [])
        job_title = recommended_jobs[0].get('job_title', 'Data Analyst') if recommended_jobs else 'Data Analyst'
        
        prompt += f"""

**M√©tier vis√© :** {job_title}

**Consignes :**
1. Identifie les 2-3 comp√©tences cl√©s √† d√©velopper en priorit√©
2. Propose un plan d'apprentissage en 3 √©tapes concr√®tes
3. Sugg√®re des ressources sp√©cifiques (cours, projets, outils)
4. Dur√©e estim√©e : 3-6 mois
5. Format : concis, actionnable, professionnel

R√©ponds en fran√ßais, style professionnel."""
        
        # Appel API avec le mod√®le d√©tect√©
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        
        plan = response.text
        
        # Sauvegarder dans le cache
        cache[cache_key] = {
            'query': prompt,
            'response': plan,
            'timestamp': datetime.now().isoformat(),
            'model_used': model_name,
            'profile_summary': {
                'weak_blocks': [w['bloc'] for w in weak_blocks],
                'target_job': job_title
            }
        }
        save_cache(cache)
        
        print(f"‚úÖ Plan g√©n√©r√© avec {model_name} et sauvegard√© dans le cache")
        
        return plan
    
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration plan : {e}")
        # Retourner un plan par d√©faut
        return """## üìö Plan de Progression Personnalis√©

**Phase 1 : Renforcement des fondamentaux (Mois 1-2)**
- R√©viser les bases de Python et analyse de donn√©es
- Pratiquer avec des datasets Kaggle
- Suivre des tutoriels sur Pandas et NumPy

**Phase 2 : D√©veloppement des comp√©tences techniques (Mois 3-4)**
- Approfondir le Machine Learning supervis√©
- R√©aliser des projets pratiques
- √âtudier les algorithmes avanc√©s

**Phase 3 : Sp√©cialisation et portfolio (Mois 5-6)**
- Se sp√©cialiser dans le domaine cibl√©
- Construire un portfolio de projets
- Participer √† des comp√©titions Kaggle

üí° *Note: Plan g√©n√©r√© automatiquement. Consultez un mentor pour personnalisation.*"""


def generate_professional_bio(analysis_results: Dict) -> str:
    """
    G√©n√©rer une bio professionnelle style Executive Summary avec CACHE
    UN SEUL APPEL API par profil unique
    
    Args:
        analysis_results: R√©sultats de l'analyse SBERT
        
    Returns:
        Bio professionnelle (str)
    """
    print("\nüìù G√©n√©ration de la Bio Professionnelle...")
    
    # Charger le cache
    cache = load_cache()
    
    # G√©n√©rer la cl√© de cache
    cache_key = generate_cache_key('bio', analysis_results)
    
    # V√©rifier si d√©j√† en cache
    if cache_key in cache:
        print("‚úÖ Bio trouv√©e dans le cache ! (Aucun appel API)")
        return cache[cache_key]['response']
    
    print("üåê Appel API Gemini (nouveau profil)...")
    
    try:
        # D√©tecter le mod√®le disponible
        model_name = detect_available_model()
        
        # Identifier les blocs FORTS (score >= 0.6)
        strong_blocks = []
        block_scores = analysis_results.get('block_scores', {})
        
        for bloc_key, bloc_data in block_scores.items():
            score = bloc_data.get('score', 0)
            if score >= 0.6:
                strong_blocks.append({
                    'bloc': bloc_key,
                    'score': score
                })
        
        # Trier par score d√©croissant
        strong_blocks = sorted(strong_blocks, key=lambda x: x['score'], reverse=True)
        
        # Construire le prompt
        bloc_names = {
            'bloc1': 'Data Analysis & Visualization',
            'bloc2': 'Machine Learning Supervis√©',
            'bloc3': 'Machine Learning Non Supervis√©',
            'bloc4': 'NLP (Natural Language Processing)',
            'bloc5': 'Statistiques & Math√©matiques'
        }
        
        prompt = f"""Tu es un expert en r√©daction de profils professionnels.

Cr√©e une bio professionnelle courte et percutante (Executive Summary style).

**Points forts d√©tect√©s :**
"""
        
        for strong in strong_blocks[:3]:
            bloc_name = bloc_names.get(strong['bloc'], strong['bloc'])
            prompt += f"\n- {bloc_name} ({strong['score']:.0%})"
        
        recommended_jobs = analysis_results.get('recommended_jobs', [])
        if recommended_jobs:
            job_title = recommended_jobs[0].get('job_title', 'Data Analyst')
            match_score = recommended_jobs[0].get('match_score', 0)
        else:
            job_title = 'Data Analyst'
            match_score = 0
        
        prompt += f"""

**Profil m√©tier recommand√© :** {job_title}
**Score de compatibilit√© :** {match_score:.1f}%

**Consignes :**
1. Longueur : 2 paragraphes (6-8 phrases au total)
2. Paragraphe 1 : Pr√©sentation du profil et comp√©tences techniques ma√Ætris√©es
3. Paragraphe 2 : Exp√©rience, projets r√©alis√©s et objectifs professionnels
4. Style : Professionnel, impactant, orient√© r√©sultats
5. Mettre en avant les points forts d√©tect√©s
6. Positionner clairement pour le m√©tier recommand√©
7. Terminer par une ouverture vers les opportunit√©s futures

R√©ponds en fran√ßais, sans titre, 2 paragraphes bien structur√©s."""
        
        # Appel API avec le mod√®le d√©tect√©
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        
        bio = response.text
        
        # Sauvegarder dans le cache
        cache[cache_key] = {
            'query': prompt,
            'response': bio,
            'timestamp': datetime.now().isoformat(),
            'model_used': model_name,
            'profile_summary': {
                'strong_blocks': [s['bloc'] for s in strong_blocks],
                'target_job': job_title
            }
        }
        save_cache(cache)
        
        print(f"‚úÖ Bio g√©n√©r√©e avec {model_name} et sauvegard√©e dans le cache")
        
        return bio
    
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration bio : {e}")
        # Retourner une bio par d√©faut
        coverage = analysis_results.get('coverage_score', 0)
        recommended_jobs = analysis_results.get('recommended_jobs', [])
        job_title = recommended_jobs[0].get('job_title', 'Data Analyst') if recommended_jobs else 'Data Analyst'
        
        return f"""Profil Data Science polyvalent avec un score de couverture de {coverage:.0%}. Comp√©tences solides en analyse de donn√©es et mod√©lisation. Orient√© {job_title} avec une forte capacit√© d'adaptation et un potentiel de croissance √©lev√©. Pr√™t √† relever de nouveaux d√©fis dans l'√©cosyst√®me data."""