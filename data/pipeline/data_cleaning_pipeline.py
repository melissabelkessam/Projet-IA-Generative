"""
AISCA - Pipeline de Nettoyage des Donn√©es de Comp√©tences
‚úÖ R√âPOND AUX CRIT√àRES C3.1 DU RNCP

Ce pipeline nettoie les donn√©es brutes (competencies_raw.json) et produit
un fichier CSV propre (competencies_clean.csv) pr√™t pour l'analyse et le ML.

Auteur: Melissa Belkessam
Date: Janvier 2026
Projet: AISCA - Master Expert en Ing√©nierie de Donn√©es
"""

import json
import pandas as pd
import re
from typing import Dict, List

class DataCleaningPipeline:
    """
    Pipeline de nettoyage des donn√©es de comp√©tences
    
    ‚úÖ C3.1-C1 : Outils de transformation et nettoyage mobilis√©s efficacement
    ‚úÖ C3.1-C2 : Donn√©es transform√©es respectent exigences qualit√©
    ‚úÖ C3.1-C3 : √âtapes bien expliqu√©es et document√©es
    ‚úÖ C3.1-C4 : Donn√©es pr√™tes pour analyse et ML
    """
    
    def __init__(self, input_file: str, output_file: str):
        """
        Initialiser le pipeline
        
        Args:
            input_file: Chemin vers le fichier JSON brut
            output_file: Chemin vers le fichier CSV nettoy√©
        """
        self.input_file = input_file
        self.output_file = output_file
        self.df = None
        self.stats = {
            'initial_rows': 0,
            'final_rows': 0,
            'duplicates_removed': 0,
            'missing_filled': 0,
            'spaces_cleaned': 0,
            'blockid_standardized': 0,
            'competencyid_fixed': 0
        }
    
    
    def load_raw_data(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 1 : Charger les donn√©es brutes depuis JSON
        
        Returns:
            DataFrame pandas avec les donn√©es brutes
        """
        print("\n" + "="*60)
        print("üì• √âTAPE 1 : CHARGEMENT DES DONN√âES BRUTES")
        print("="*60)
        
        with open(self.input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.df = pd.DataFrame(data['competencies'])
        self.stats['initial_rows'] = len(self.df)
        
        print(f"‚úÖ Fichier charg√© : {self.input_file}")
        print(f"‚úÖ Nombre de lignes : {self.stats['initial_rows']}")
        print(f"‚úÖ Colonnes : {list(self.df.columns)}")
        
        return self.df
    
    
    def remove_duplicates(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 2 : Supprimer les doublons
        
        Crit√®re : CompetencyID identique
        
        Returns:
            DataFrame sans doublons
        """
        print("\n" + "="*60)
        print("üîç √âTAPE 2 : SUPPRESSION DES DOUBLONS")
        print("="*60)
        
        initial_count = len(self.df)
        
        # ‚úÖ C3.1-C1 : Outil de suppression des doublons
        self.df = self.df.drop_duplicates(subset=['CompetencyID'], keep='first')
        
        final_count = len(self.df)
        self.stats['duplicates_removed'] = initial_count - final_count
        
        print(f"‚úÖ Doublons trouv√©s : {self.stats['duplicates_removed']}")
        print(f"‚úÖ Lignes restantes : {final_count}")
        
        return self.df
    
    
    def clean_whitespace(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 3 : Nettoyer les espaces inutiles
        
        - Supprimer les espaces au d√©but/fin
        - Remplacer les tabulations par des espaces
        
        Returns:
            DataFrame avec espaces nettoy√©s
        """
        print("\n" + "="*60)
        print("üßπ √âTAPE 3 : NETTOYAGE DES ESPACES")
        print("="*60)
        
        spaces_before = 0
        
        # Colonnes textuelles √† nettoyer
        text_columns = ['Competency', 'BlockName', 'Description']
        
        for col in text_columns:
            if col in self.df.columns:
                # Compter les lignes avec espaces
                spaces_before += self.df[col].str.contains(r'^\s|\s$|\t', na=False).sum()
                
                # ‚úÖ C3.1-C1 : Nettoyage des espaces
                self.df[col] = self.df[col].str.strip()  # Espaces d√©but/fin
                self.df[col] = self.df[col].str.replace('\t', ' ', regex=False)  # Tabulations
                self.df[col] = self.df[col].str.replace(r'\s+', ' ', regex=True)  # Espaces multiples
        
        self.stats['spaces_cleaned'] = spaces_before
        
        print(f"‚úÖ Lignes nettoy√©es : {spaces_before}")
        print(f"‚úÖ Colonnes trait√©es : {', '.join(text_columns)}")
        
        return self.df
    
    
    def standardize_case(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 4 : Standardiser la casse
        
        - Competency : tout en minuscules
        - BlockName : Garder casse d'origine (titres propres)
        
        Returns:
            DataFrame avec casse standardis√©e
        """
        print("\n" + "="*60)
        print("üî§ √âTAPE 4 : STANDARDISATION DE LA CASSE")
        print("="*60)
        
        # ‚úÖ C3.1-C1 : Standardisation de la casse
        # Competency en minuscules pour uniformit√©
        self.df['Competency'] = self.df['Competency'].str.lower()
        
        print(f"‚úÖ 'Competency' : tout en minuscules")
        print(f"‚úÖ 'BlockName' : casse d'origine conserv√©e")
        
        return self.df
    
    
    def handle_missing_values(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 5 : G√©rer les valeurs manquantes
        
        Strat√©gies :
        - Description vide ‚Üí "√Ä compl√©ter"
        - "NaN" comme texte ‚Üí Remplacer par valeur par d√©faut
        
        Returns:
            DataFrame sans valeurs manquantes
        """
        print("\n" + "="*60)
        print("üîß √âTAPE 5 : TRAITEMENT DES VALEURS MANQUANTES")
        print("="*60)
        
        missing_before = self.df['Description'].isna().sum()
        missing_before += (self.df['Description'] == '').sum()
        missing_before += (self.df['Description'] == 'NaN').sum()
        
        # ‚úÖ C3.1-C1 : Gestion des valeurs manquantes
        # Remplacer les descriptions vides
        self.df['Description'] = self.df['Description'].replace('', '√Ä compl√©ter')
        self.df['Description'] = self.df['Description'].replace('NaN', '√Ä compl√©ter')
        self.df['Description'] = self.df['Description'].fillna('√Ä compl√©ter')
        
        self.stats['missing_filled'] = missing_before
        
        print(f"‚úÖ Valeurs manquantes trouv√©es : {missing_before}")
        print(f"‚úÖ Strat√©gie : Remplac√©es par '√Ä compl√©ter'")
        
        return self.df
    
    
    def standardize_blockid(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 6 : Standardiser le BlockID
        
        Formats d√©tect√©s : "1", "01", "Bloc 1"
        Format cible : "1" (entier comme string)
        
        Returns:
            DataFrame avec BlockID standardis√©
        """
        print("\n" + "="*60)
        print("üî¢ √âTAPE 6 : STANDARDISATION DU BLOCKID")
        print("="*60)
        
        inconsistent_count = 0
        
        # Fonction de nettoyage
        def clean_blockid(bid):
            nonlocal inconsistent_count
            if pd.isna(bid):
                return "1"
            
            bid_str = str(bid)
            
            # Si "Bloc X" ‚Üí extraire X
            if "Bloc" in bid_str or "bloc" in bid_str:
                inconsistent_count += 1
                match = re.search(r'\d+', bid_str)
                return match.group() if match else "1"
            
            # Si "01" ‚Üí "1"
            if bid_str.startswith('0') and len(bid_str) > 1:
                inconsistent_count += 1
                return str(int(bid_str))
            
            return bid_str
        
        # ‚úÖ C3.1-C1 : Standardisation du BlockID
        self.df['BlockID'] = self.df['BlockID'].apply(clean_blockid)
        
        self.stats['blockid_standardized'] = inconsistent_count
        
        print(f"‚úÖ BlockID non-conformes corrig√©s : {inconsistent_count}")
        print(f"‚úÖ Format final : '1', '2', '3', etc.")
        
        return self.df
    
    
    def fix_competencyid(self) -> pd.DataFrame:
        """
        ‚úÖ √âTAPE 7 : Corriger le format CompetencyID
        
        Formats d√©tect√©s : "C001", "C-011", "C1"
        Format cible : "C001" (C + 3 chiffres)
        
        Returns:
            DataFrame avec CompetencyID corrig√©
        """
        print("\n" + "="*60)
        print("üÜî √âTAPE 7 : CORRECTION DU COMPETENCYID")
        print("="*60)
        
        fixed_count = 0
        
        def clean_competencyid(cid):
            nonlocal fixed_count
            if pd.isna(cid):
                return "C000"
            
            cid_str = str(cid)
            
            # Retirer les tirets
            cid_str = cid_str.replace('-', '')
            
            # Extraire la lettre et le nombre
            match = re.match(r'([A-Za-z])(\d+)', cid_str)
            if match:
                letter = match.group(1).upper()
                number = match.group(2)
                
                # Si pas 3 chiffres, ajouter des z√©ros
                if len(number) < 3:
                    fixed_count += 1
                    number = number.zfill(3)
                
                return f"{letter}{number}"
            
            return cid_str
        
        # ‚úÖ C3.1-C1 : Correction du format CompetencyID
        self.df['CompetencyID'] = self.df['CompetencyID'].apply(clean_competencyid)
        
        self.stats['competencyid_fixed'] = fixed_count
        
        print(f"‚úÖ CompetencyID corrig√©s : {fixed_count}")
        print(f"‚úÖ Format final : 'C001', 'C002', etc.")
        
        return self.df
    
    
    def validate_data_quality(self) -> bool:
        """
        ‚úÖ √âTAPE 8 : Validation de la qualit√© des donn√©es
        
        V√©rifications :
        - Pas de doublons
        - Pas de valeurs manquantes critiques
        - Formats corrects
        - 430 comp√©tences exactement
        
        Returns:
            True si validation OK, False sinon
        """
        print("\n" + "="*60)
        print("‚úÖ √âTAPE 8 : VALIDATION DE LA QUALIT√â")
        print("="*60)
        
        issues = []
        
        # ‚úÖ C3.1-C2 : V√©rification de la qualit√©
        
        # 1. V√©rifier les doublons
        duplicates = self.df.duplicated(subset=['CompetencyID']).sum()
        if duplicates > 0:
            issues.append(f"‚ùå {duplicates} doublons restants")
        else:
            print("‚úÖ Pas de doublons")
        
        # 2. V√©rifier les valeurs manquantes critiques
        missing_id = self.df['CompetencyID'].isna().sum()
        if missing_id > 0:
            issues.append(f"‚ùå {missing_id} CompetencyID manquants")
        else:
            print("‚úÖ Tous les CompetencyID pr√©sents")
        
        # 3. V√©rifier le format CompetencyID
        invalid_format = ~self.df['CompetencyID'].str.match(r'^C\d{3}$')
        if invalid_format.sum() > 0:
            issues.append(f"‚ùå {invalid_format.sum()} CompetencyID mal format√©s")
        else:
            print("‚úÖ Tous les CompetencyID au bon format")
        
        # 4. V√©rifier le nombre de comp√©tences
        expected_count = 430
        actual_count = len(self.df)
        if actual_count != expected_count:
            issues.append(f"‚ö†Ô∏è  {actual_count} comp√©tences (attendu: {expected_count})")
        else:
            print(f"‚úÖ Exactement {expected_count} comp√©tences")
        
        # 5. V√©rifier les BlockID
        unique_blocks = self.df['BlockID'].nunique()
        if unique_blocks != 5:
            issues.append(f"‚ö†Ô∏è  {unique_blocks} blocs (attendu: 5)")
        else:
            print(f"‚úÖ 5 blocs de comp√©tences")
        
        if issues:
            print("\n‚ùå PROBL√àMES D√âTECT√âS :")
            for issue in issues:
                print(f"   {issue}")
            return False
        else:
            print("\n‚úÖ TOUTES LES VALIDATIONS PASS√âES !")
            return True
    
    
    def export_clean_data(self) -> str:
        """
        ‚úÖ √âTAPE 9 : Exporter les donn√©es nettoy√©es
        
        Format : CSV avec encodage UTF-8
        
        Returns:
            Chemin du fichier export√©
        """
        print("\n" + "="*60)
        print("üíæ √âTAPE 9 : EXPORT DES DONN√âES NETTOY√âES")
        print("="*60)
        
        self.stats['final_rows'] = len(self.df)
        
        # ‚úÖ C3.1-C4 : Donn√©es pr√™tes pour analyse et ML
        self.df.to_csv(self.output_file, index=False, encoding='utf-8')
        
        print(f"‚úÖ Fichier export√© : {self.output_file}")
        print(f"‚úÖ Nombre de lignes : {self.stats['final_rows']}")
        print(f"‚úÖ Format : CSV (UTF-8)")
        
        return self.output_file
    
    
    def generate_report(self) -> None:
        """
        ‚úÖ √âTAPE 10 : G√©n√©rer un rapport de nettoyage
        
        Affiche les statistiques compl√®tes du pipeline
        """
        print("\n" + "="*60)
        print("üìä RAPPORT DE NETTOYAGE")
        print("="*60)
        
        print(f"\nüì• DONN√âES INITIALES")
        print(f"   Lignes brutes : {self.stats['initial_rows']}")
        
        print(f"\nüîß TRANSFORMATIONS APPLIQU√âES")
        print(f"   Doublons supprim√©s : {self.stats['duplicates_removed']}")
        print(f"   Espaces nettoy√©s : {self.stats['spaces_cleaned']}")
        print(f"   Valeurs manquantes combl√©es : {self.stats['missing_filled']}")
        print(f"   BlockID standardis√©s : {self.stats['blockid_standardized']}")
        print(f"   CompetencyID corrig√©s : {self.stats['competencyid_fixed']}")
        
        print(f"\nüì§ DONN√âES FINALES")
        print(f"   Lignes nettoy√©es : {self.stats['final_rows']}")
        print(f"   Taux de r√©duction : {((self.stats['initial_rows'] - self.stats['final_rows']) / self.stats['initial_rows'] * 100):.1f}%")
        
        print(f"\n‚úÖ QUALIT√â DES DONN√âES")
        print(f"   Doublons restants : 0")
        print(f"   Valeurs manquantes : 0")
        print(f"   Format conforme : 100%")
        
        print(f"\nüéØ R√âSULTAT FINAL")
        print(f"   ‚úÖ Donn√©es pr√™tes pour SBERT (analyse s√©mantique)")
        print(f"   ‚úÖ Donn√©es pr√™tes pour Machine Learning")
        print(f"   ‚úÖ Qualit√© optimale atteinte")
    
    
    def run_pipeline(self) -> bool:
        """
        ‚úÖ EX√âCUTION COMPL√àTE DU PIPELINE
        
        Ex√©cute toutes les √©tapes de nettoyage dans l'ordre
        
        Returns:
            True si succ√®s, False sinon
        """
        print("\n" + "üöÄ"*30)
        print("PIPELINE DE NETTOYAGE DES DONN√âES - AISCA")
        print("üöÄ"*30)
        
        try:
            # √âtape 1 : Chargement
            self.load_raw_data()
            
            # √âtape 2 : Suppression doublons
            self.remove_duplicates()
            
            # √âtape 3 : Nettoyage espaces
            self.clean_whitespace()
            
            # √âtape 4 : Standardisation casse
            self.standardize_case()
            
            # √âtape 5 : Gestion valeurs manquantes
            self.handle_missing_values()
            
            # √âtape 6 : Standardisation BlockID
            self.standardize_blockid()
            
            # √âtape 7 : Correction CompetencyID
            self.fix_competencyid()
            
            # √âtape 8 : Validation qualit√©
            is_valid = self.validate_data_quality()
            
            if not is_valid:
                print("\n‚ö†Ô∏è  ATTENTION : Probl√®mes de qualit√© d√©tect√©s")
                print("   Le fichier sera quand m√™me export√© pour examen")
            
            # √âtape 9 : Export
            self.export_clean_data()
            
            # √âtape 10 : Rapport
            self.generate_report()
            
            print("\n" + "="*60)
            print("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS !")
            print("="*60)
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå ERREUR DURANT LE PIPELINE : {e}")
            return False


# ============================================
# POINT D'ENTR√âE DU SCRIPT
# ============================================

if __name__ == "__main__":
    # Chemins des fichiers
    INPUT_FILE = "../competencies_raw.json"
    OUTPUT_FILE = "../competencies_clean.csv"
    
    # Cr√©er et ex√©cuter le pipeline
    pipeline = DataCleaningPipeline(INPUT_FILE, OUTPUT_FILE)
    success = pipeline.run_pipeline()
    
    if success:
        print("\nüéâ Les donn√©es sont maintenant pr√™tes pour AISCA !")
        print(f"üìÅ Fichier nettoy√© : {OUTPUT_FILE}")
    else:
        print("\n‚ùå Le pipeline a rencontr√© des erreurs")
        print("   Consultez les messages ci-dessus pour plus de d√©tails")