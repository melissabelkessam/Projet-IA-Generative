[
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T15:04:24.203941Z",
    "responses": {
      "bloc1_likert": 4,
      "bloc1_text": "projet data dev",
      "bloc1_yesno": "Oui",
      "bloc1_choice": "Visualisation",
      "bloc1_checks": [
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 2,
      "bloc2_text": "projet data dev",
      "bloc2_yesno": "Oui",
      "bloc2_choice": "Clustering",
      "bloc2_checks": [
        "XGBoost"
      ],
      "bloc3_likert": 3,
      "bloc3_text": "projet data dev",
      "bloc3_yesno": "Oui",
      "bloc3_choice": "Tokenization",
      "bloc3_checks": [
        "NLTK"
      ],
      "bloc4_likert": 3,
      "bloc4_text": "projet data dev",
      "bloc4_yesno": "Oui",
      "bloc4_choice": "Probabilités",
      "bloc4_checks": [
        "SciPy"
      ],
      "bloc5_likert": 3,
      "bloc5_text": "projet data dev",
      "bloc5_yesno": "Oui",
      "bloc5_choice": "Spark",
      "bloc5_checks": [
        "AWS"
      ],
      "bloc6_likert": 3,
      "bloc6_text": "projet data dev",
      "bloc6_yesno": "Oui",
      "bloc6_choice": "Dashboard",
      "bloc6_checks": [
        "Slides"
      ],
      "bloc7_likert": 3,
      "bloc7_text": "projet data dev",
      "bloc7_yesno": "Oui",
      "bloc7_choice": "RGPD",
      "bloc7_checks": [
        "RBAC"
      ],
      "bloc8_likert": 3,
      "bloc8_text": "projet data dev",
      "bloc8_yesno": "Oui",
      "bloc8_choice": "JOIN/CTE",
      "bloc8_checks": [
        "MySQL"
      ],
      "bloc9_likert": 3,
      "bloc9_text": "projet data dev",
      "bloc9_yesno": "Oui",
      "bloc9_choice": "CI/CD",
      "bloc9_checks": [
        "MLflow"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T16:06:20.500604Z",
    "responses": {
      "bloc1_likert": 3,
      "bloc1_text": "projet dev data",
      "bloc1_yesno": "Oui",
      "bloc1_choice": "Nettoyage",
      "bloc1_checks": [
        "Plotly"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "projet dev data",
      "bloc2_yesno": "Oui",
      "bloc2_choice": "Régression",
      "bloc2_checks": [
        "PyTorch"
      ],
      "bloc3_likert": 3,
      "bloc3_text": "projet dev data",
      "bloc3_yesno": "Oui",
      "bloc3_choice": "Tokenization",
      "bloc3_checks": [
        "NLTK"
      ],
      "bloc4_likert": 3,
      "bloc4_text": "projet dev data",
      "bloc4_yesno": "Oui",
      "bloc4_choice": "Probabilités",
      "bloc4_checks": [
        "R"
      ],
      "bloc5_likert": 5,
      "bloc5_text": "projet dev data",
      "bloc5_yesno": "Oui",
      "bloc5_choice": "Spark",
      "bloc5_checks": [
        "GCP"
      ],
      "bloc6_likert": 4,
      "bloc6_text": "projet dev data",
      "bloc6_yesno": "Oui",
      "bloc6_choice": "Dashboard",
      "bloc6_checks": [
        "Tableau"
      ],
      "bloc7_likert": 3,
      "bloc7_text": "projet dev data",
      "bloc7_yesno": "Oui",
      "bloc7_choice": "RGPD",
      "bloc7_checks": [
        "Audit logs"
      ],
      "bloc8_likert": 3,
      "bloc8_text": "projet dev data",
      "bloc8_yesno": "Oui",
      "bloc8_choice": "JOIN/CTE",
      "bloc8_checks": [
        "MongoDB"
      ],
      "bloc9_likert": 3,
      "bloc9_text": "projet dev data",
      "bloc9_yesno": "Oui",
      "bloc9_choice": "CI/CD",
      "bloc9_checks": [
        "FastAPI"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T20:46:45.691650Z",
    "responses": {
      "bloc1_likert": 3,
      "bloc1_text": "projet data analyst",
      "bloc1_yesno": "Oui",
      "bloc1_choice": "Nettoyage",
      "bloc1_checks": [
        "NumPy"
      ],
      "bloc2_likert": 3,
      "bloc2_text": "projet data analyst",
      "bloc2_yesno": "Oui",
      "bloc2_choice": "Régression",
      "bloc2_checks": [
        "TensorFlow"
      ],
      "bloc3_likert": 3,
      "bloc3_text": "non jamais",
      "bloc3_yesno": "Oui",
      "bloc3_choice": "Tokenization",
      "bloc3_checks": [
        "SentenceTransformers"
      ],
      "bloc4_likert": 3,
      "bloc4_text": "projet data analyst",
      "bloc4_yesno": "Oui",
      "bloc4_choice": "Probabilités",
      "bloc4_checks": [
        "SciPy",
        "SymPy"
      ],
      "bloc5_likert": 3,
      "bloc5_text": "projet data analyst",
      "bloc5_yesno": "Oui",
      "bloc5_choice": "Spark",
      "bloc5_checks": [
        "GCP"
      ],
      "bloc6_likert": 3,
      "bloc6_text": "non jamais",
      "bloc6_yesno": "Oui",
      "bloc6_choice": "Dashboard",
      "bloc6_checks": [
        "Tableau"
      ],
      "bloc7_likert": 4,
      "bloc7_text": "projet data analyst",
      "bloc7_yesno": "Oui",
      "bloc7_choice": "RGPD",
      "bloc7_checks": [
        "RBAC"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "projet data analyst",
      "bloc8_yesno": "Oui",
      "bloc8_choice": "JOIN/CTE",
      "bloc8_checks": [
        "PostgreSQL",
        "MySQL"
      ],
      "bloc9_likert": 5,
      "bloc9_text": "projet data analyst",
      "bloc9_yesno": "Oui",
      "bloc9_choice": "CI/CD",
      "bloc9_checks": [
        "DVC"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:00:12.963094Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "projet dev data",
      "bloc1_yesno": "Oui",
      "bloc1_choice": "Nettoyage",
      "bloc1_checks": [
        "Pandas"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "projet dev data",
      "bloc2_yesno": "Oui",
      "bloc2_choice": "Régression",
      "bloc2_checks": [
        "XGBoost"
      ],
      "bloc3_likert": 1,
      "bloc3_text": "non",
      "bloc3_yesno": "Oui",
      "bloc3_choice": "Tokenization",
      "bloc3_checks": [
        "SentenceTransformers"
      ],
      "bloc4_likert": 4,
      "bloc4_text": "etude scolaire",
      "bloc4_yesno": "Oui",
      "bloc4_choice": "Probabilités",
      "bloc4_checks": [
        "Statsmodels",
        "R"
      ],
      "bloc5_likert": 1,
      "bloc5_text": "nn",
      "bloc5_yesno": "Oui",
      "bloc5_choice": "Spark",
      "bloc5_checks": [
        "Azure"
      ],
      "bloc6_likert": 5,
      "bloc6_text": "projet dev data",
      "bloc6_yesno": "Oui",
      "bloc6_choice": "Dashboard",
      "bloc6_checks": [
        "Excel",
        "Slides",
        "PowerBI",
        "Tableau"
      ],
      "bloc7_likert": 2,
      "bloc7_text": "projet dev data",
      "bloc7_yesno": "Oui",
      "bloc7_choice": "RGPD",
      "bloc7_checks": [
        "Data Catalog"
      ],
      "bloc8_likert": 4,
      "bloc8_text": "projet dev data",
      "bloc8_yesno": "Oui",
      "bloc8_choice": "JOIN/CTE",
      "bloc8_checks": [
        "MySQL"
      ],
      "bloc9_likert": 2,
      "bloc9_text": "non",
      "bloc9_yesno": "Oui",
      "bloc9_choice": "CI/CD",
      "bloc9_checks": [
        "DVC"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:02:54.250757Z",
    "responses": {
      "bloc1_likert": 3,
      "bloc1_text": "projet dev data",
      "bloc1_yesno": "Oui",
      "bloc1_choice": "Nettoyage",
      "bloc1_checks": [],
      "bloc2_likert": 1,
      "bloc2_text": "non",
      "bloc2_yesno": "Oui",
      "bloc2_choice": "Régression",
      "bloc2_checks": [],
      "bloc3_likert": 1,
      "bloc3_text": "non",
      "bloc3_yesno": "Oui",
      "bloc3_choice": "Tokenization",
      "bloc3_checks": [],
      "bloc4_likert": 1,
      "bloc4_text": "non",
      "bloc4_yesno": "Oui",
      "bloc4_choice": "Probabilités",
      "bloc4_checks": [],
      "bloc5_likert": 1,
      "bloc5_text": "non",
      "bloc5_yesno": "Oui",
      "bloc5_choice": "Spark",
      "bloc5_checks": [],
      "bloc6_likert": 1,
      "bloc6_text": "non",
      "bloc6_yesno": "Oui",
      "bloc6_choice": "Dashboard",
      "bloc6_checks": [],
      "bloc7_likert": 1,
      "bloc7_text": "non",
      "bloc7_yesno": "Oui",
      "bloc7_choice": "RGPD",
      "bloc7_checks": [],
      "bloc8_likert": 1,
      "bloc8_text": "non",
      "bloc8_yesno": "Oui",
      "bloc8_choice": "JOIN/CTE",
      "bloc8_checks": [],
      "bloc9_likert": 1,
      "bloc9_text": "non",
      "bloc9_yesno": "Oui",
      "bloc9_choice": "CI/CD",
      "bloc9_checks": []
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:11:25.898764Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "projet data en alternance",
      "bloc1_yesno": "Oui",
      "bloc1_choice": "Nettoyage",
      "bloc1_checks": [
        "NumPy",
        "Pandas",
        "Matplotlib"
      ],
      "bloc2_likert": 1,
      "bloc2_text": "base en ça",
      "bloc2_yesno": "Oui",
      "bloc2_choice": "Classification",
      "bloc2_checks": [
        "scikit-learn"
      ],
      "bloc3_likert": 4,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_choice": "RAG",
      "bloc3_checks": [
        "Transformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "ecole",
      "bloc4_yesno": "Non",
      "bloc4_choice": "Régression",
      "bloc4_checks": [
        "SymPy"
      ],
      "bloc5_likert": 2,
      "bloc5_text": "jsp",
      "bloc5_yesno": "Oui",
      "bloc5_choice": "Hadoop",
      "bloc5_checks": [
        "GCP",
        "Azure"
      ],
      "bloc6_likert": 2,
      "bloc6_text": "jsp",
      "bloc6_yesno": "Oui",
      "bloc6_choice": "Dashboard",
      "bloc6_checks": [],
      "bloc7_likert": 0,
      "bloc7_text": "jsp",
      "bloc7_yesno": "Oui",
      "bloc7_choice": "Aucun / Je ne connais pas",
      "bloc7_checks": [],
      "bloc8_likert": 3,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_choice": "JOIN/CTE",
      "bloc8_checks": [
        "MySQL"
      ],
      "bloc9_likert": 2,
      "bloc9_text": "projet dev",
      "bloc9_yesno": "Oui",
      "bloc9_choice": "CI/CD",
      "bloc9_checks": [
        "MLflow",
        "FastAPI"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:25:47.057796Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "projet data",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "Pandas",
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 2,
      "bloc2_text": "projet data",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Clustering"
      ],
      "bloc2_tools": [
        "PyTorch"
      ],
      "bloc3_likert": 2,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "RAG (Retrieval-Augmented Generation)"
      ],
      "bloc3_tools": [
        "Transformers"
      ],
      "bloc4_likert": 3,
      "bloc4_text": "projet data",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Probabilités",
        "Tests statistiques"
      ],
      "bloc4_tools": [
        "SciPy"
      ],
      "bloc5_likert": 2,
      "bloc5_text": "projet data",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Apache Spark"
      ],
      "bloc5_tools": [
        "Azure"
      ],
      "bloc6_likert": 3,
      "bloc6_text": "projet data",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Data storytelling"
      ],
      "bloc6_tools": [
        "Google Slides",
        "Tableau",
        "PowerBI",
        "Excel"
      ],
      "bloc7_likert": 0,
      "bloc7_text": "projet data",
      "bloc7_yesno": "Non",
      "bloc7_tasks": [
        "Aucune de ces tâches"
      ],
      "bloc7_tools": [],
      "bloc8_likert": 2,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Window functions"
      ],
      "bloc8_tools": [
        "SQLite",
        "MongoDB"
      ],
      "bloc9_likert": 3,
      "bloc9_text": "projet data",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "CI/CD pour ML"
      ],
      "bloc9_tools": [
        "Airflow"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:30:05.208661Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "projet data",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Visualisation",
        "Nettoyage de données",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "Pandas",
        "Matplotlib",
        "NumPy"
      ],
      "bloc2_likert": 2,
      "bloc2_text": "projet data",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Classification"
      ],
      "bloc2_tools": [
        "scikit-learn"
      ],
      "bloc3_likert": 3,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "RAG (Retrieval-Augmented Generation)",
        "Tokenization",
        "Classification de texte"
      ],
      "bloc3_tools": [
        "Transformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "lecole",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Régression statistique",
        "Optimisation mathématique",
        "Probabilités",
        "Tests statistiques"
      ],
      "bloc4_tools": [
        "SymPy",
        "R"
      ],
      "bloc5_likert": 2,
      "bloc5_text": "projet data",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Databricks"
      ],
      "bloc5_tools": [
        "AWS"
      ],
      "bloc6_likert": 2,
      "bloc6_text": "projet data",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Data storytelling"
      ],
      "bloc6_tools": [
        "Excel",
        "PowerBI",
        "Tableau",
        "Google Slides"
      ],
      "bloc7_likert": 1,
      "bloc7_text": "jsp",
      "bloc7_yesno": "Non",
      "bloc7_tasks": [],
      "bloc7_tools": [],
      "bloc8_likert": 4,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Requêtes complexes (JOIN, CTE)"
      ],
      "bloc8_tools": [
        "MySQL"
      ],
      "bloc9_likert": 2,
      "bloc9_text": "projet data",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "CI/CD pour ML"
      ],
      "bloc9_tools": [
        "FastAPI"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:41:59.621157Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "projet data",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Visualisation",
        "Analyse exploratoire (EDA)",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Pandas",
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 2,
      "bloc2_text": "non",
      "bloc2_yesno": "Non",
      "bloc2_tasks": [
        "Classification",
        "Clustering"
      ],
      "bloc2_tools": [
        "XGBoost"
      ],
      "bloc3_likert": 3,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Tokenization",
        "Classification de texte",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "bloc3_tools": [
        "NLTK"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "projet data",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Probabilités",
        "Tests statistiques",
        "Optimisation mathématique",
        "Régression statistique"
      ],
      "bloc4_tools": [
        "Statsmodels",
        "SymPy",
        "R"
      ],
      "bloc5_likert": 2,
      "bloc5_text": "projet data",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Hadoop",
        "Kafka"
      ],
      "bloc5_tools": [
        "Azure",
        "Docker"
      ],
      "bloc6_likert": 3,
      "bloc6_text": "projet data",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Présentations business",
        "Data storytelling",
        "Aucune de ces tâches"
      ],
      "bloc6_tools": [],
      "bloc7_likert": 1,
      "bloc7_text": "projet data",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "RGPD / Conformité"
      ],
      "bloc7_tools": [
        "RBAC",
        "Data Catalog"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Requêtes complexes (JOIN, CTE)",
        "Window functions",
        "Modélisation de bases de données",
        "Optimisation de requêtes"
      ],
      "bloc8_tools": [
        "MySQL",
        "SQLite",
        "PostgreSQL",
        "MongoDB"
      ],
      "bloc9_likert": 2,
      "bloc9_text": "projet data",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "CI/CD pour ML",
        "Déploiement de modèles"
      ],
      "bloc9_tools": [
        "MLflow"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-12T21:50:35.439959Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "projet data",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Feature engineering",
        "Visualisation"
      ],
      "bloc1_tools": [
        "NumPy",
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 2,
      "bloc2_text": "projet data",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Clustering"
      ],
      "bloc2_tools": [
        "scikit-learn"
      ],
      "bloc3_likert": 3,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Embeddings"
      ],
      "bloc3_tools": [
        "Transformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "projet data",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Régression statistique",
        "Tests statistiques",
        "Optimisation mathématique",
        "Probabilités"
      ],
      "bloc4_tools": [
        "SymPy",
        "R"
      ],
      "bloc5_likert": 4,
      "bloc5_text": "projet data",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Kafka",
        "Databricks"
      ],
      "bloc5_tools": [
        "Azure",
        "Docker"
      ],
      "bloc6_likert": 3,
      "bloc6_text": "projet data",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Définition de KPIs",
        "Présentations business",
        "Data storytelling"
      ],
      "bloc6_tools": [
        "PowerBI",
        "Excel",
        "Tableau",
        "Google Slides"
      ],
      "bloc7_likert": 2,
      "bloc7_text": "projet data",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "Détection de biais IA"
      ],
      "bloc7_tools": [
        "Data Catalog"
      ],
      "bloc8_likert": 2,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Optimisation de requêtes",
        "Modélisation de bases de données",
        "Requêtes complexes (JOIN, CTE)",
        "Window functions"
      ],
      "bloc8_tools": [
        "MySQL"
      ],
      "bloc9_likert": 2,
      "bloc9_text": "projet data",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [],
      "bloc9_tools": []
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T08:31:18.586705Z",
    "responses": {
      "bloc1_likert": 4,
      "bloc1_text": "projet dev",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Analyse exploratoire (EDA)",
        "Nettoyage de données",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Matplotlib",
        "Pandas",
        "Plotly"
      ],
      "bloc2_likert": 4,
      "bloc2_text": "projet dev",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Clustering",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "XGBoost",
        "PyTorch",
        "scikit-learn",
        "TensorFlow"
      ],
      "bloc3_likert": 2,
      "bloc3_text": "projet dev",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Embeddings"
      ],
      "bloc3_tools": [
        "Transformers",
        "SentenceTransformers",
        "spaCy"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "projet dev",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Régression statistique",
        "Tests statistiques",
        "Probabilités",
        "Optimisation mathématique"
      ],
      "bloc4_tools": [
        "SymPy",
        "SciPy",
        "Statsmodels",
        "R"
      ],
      "bloc5_likert": 2,
      "bloc5_text": "projet dev",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [],
      "bloc5_tools": [],
      "bloc6_likert": 2,
      "bloc6_text": "projet dev",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [],
      "bloc6_tools": [],
      "bloc7_likert": 2,
      "bloc7_text": "projet dev",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [],
      "bloc7_tools": [],
      "bloc8_likert": 2,
      "bloc8_text": "projet dev",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [],
      "bloc8_tools": [],
      "bloc9_likert": 2,
      "bloc9_text": "projet dev",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [],
      "bloc9_tools": []
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T09:27:32.699733Z",
    "responses": {
      "bloc1_likert": 4,
      "bloc1_text": "projet data",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Visualisation",
        "Analyse exploratoire (EDA)",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Matplotlib",
        "Pandas",
        "Plotly"
      ],
      "bloc2_likert": 2,
      "bloc2_text": "projet data",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Classification",
        "Clustering",
        "Systèmes de recommandation",
        "Régression"
      ],
      "bloc2_tools": [
        "XGBoost"
      ],
      "bloc3_likert": 2,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [],
      "bloc3_tools": [],
      "bloc4_likert": 2,
      "bloc4_text": "projet data",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [],
      "bloc4_tools": [],
      "bloc5_likert": 0,
      "bloc5_text": "projet data",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [],
      "bloc5_tools": [],
      "bloc6_likert": 0,
      "bloc6_text": "projet data",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [],
      "bloc6_tools": [],
      "bloc7_likert": 0,
      "bloc7_text": "projet data",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [],
      "bloc7_tools": [],
      "bloc8_likert": 0,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [],
      "bloc8_tools": [],
      "bloc9_likert": 0,
      "bloc9_text": "projet data",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [],
      "bloc9_tools": []
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T09:40:45.441561Z",
    "responses": {
      "bloc1_likert": 2,
      "bloc1_text": "projet data",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Matplotlib",
        "Pandas"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "projet data",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Classification",
        "Clustering",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "PyTorch",
        "XGBoost",
        "TensorFlow"
      ],
      "bloc3_likert": 5,
      "bloc3_text": "projet data",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Autre",
        "Tokenization",
        "Embeddings"
      ],
      "bloc3_tools": [
        "SentenceTransformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "projet data",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [],
      "bloc4_tools": [],
      "bloc5_likert": 5,
      "bloc5_text": "projet data",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Aucune de ces tâches",
        "Hadoop",
        "Databricks"
      ],
      "bloc5_tools": [
        "Azure"
      ],
      "bloc6_likert": 5,
      "bloc6_text": "projet data",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Data storytelling",
        "Présentations business"
      ],
      "bloc6_tools": [],
      "bloc7_likert": 5,
      "bloc7_text": "projet data",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "Qualité des données",
        "Traçabilité",
        "Détection de biais IA",
        "RGPD / Conformité"
      ],
      "bloc7_tools": [
        "RBAC"
      ],
      "bloc8_likert": 4,
      "bloc8_text": "projet data",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [],
      "bloc8_tools": [],
      "bloc9_likert": 4,
      "bloc9_text": "projet data",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [],
      "bloc9_tools": []
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T09:43:43.653265Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "Pandas",
        "NumPy",
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Clustering",
        "Classification",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "PyTorch",
        "XGBoost",
        "scikit-learn"
      ],
      "bloc3_likert": 5,
      "bloc3_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "RAG (Retrieval-Augmented Generation)",
        "Embeddings",
        "Tokenization",
        "Classification de texte"
      ],
      "bloc3_tools": [
        "NLTK",
        "Transformers",
        "spaCy",
        "SentenceTransformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Tests statistiques",
        "Régression statistique",
        "Optimisation mathématique",
        "Probabilités"
      ],
      "bloc4_tools": [
        "SciPy",
        "SymPy",
        "Statsmodels"
      ],
      "bloc5_likert": 5,
      "bloc5_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Apache Spark",
        "Kafka",
        "Hadoop",
        "Databricks"
      ],
      "bloc5_tools": [
        "GCP",
        "Azure",
        "AWS",
        "Docker"
      ],
      "bloc6_likert": 5,
      "bloc6_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Data storytelling",
        "Définition de KPIs",
        "Présentations business"
      ],
      "bloc6_tools": [
        "Excel",
        "Google Slides",
        "Tableau",
        "PowerBI"
      ],
      "bloc7_likert": 5,
      "bloc7_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "RGPD / Conformité",
        "Qualité des données",
        "Traçabilité",
        "Détection de biais IA"
      ],
      "bloc7_tools": [
        "RBAC",
        "Audit logs",
        "Outils d'anonymisation",
        "Data Catalog"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Requêtes complexes (JOIN, CTE)",
        "Aucune de ces tâches",
        "Window functions",
        "Modélisation de bases de données",
        "Optimisation de requêtes"
      ],
      "bloc8_tools": [
        "SQLite",
        "MySQL",
        "PostgreSQL"
      ],
      "bloc9_likert": 5,
      "bloc9_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "Déploiement de modèles",
        "CI/CD pour ML",
        "Monitoring de modèles",
        "MLflow / DVC"
      ],
      "bloc9_tools": [
        "DVC",
        "Airflow",
        "MLflow",
        "FastAPI"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T09:46:55.471644Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "Pandas",
        "NumPy",
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Classification",
        "Clustering",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "scikit-learn",
        "XGBoost",
        "PyTorch",
        "TensorFlow"
      ],
      "bloc3_likert": 5,
      "bloc3_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Tokenization",
        "Classification de texte",
        "Embeddings",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "bloc3_tools": [
        "NLTK",
        "spaCy",
        "Transformers",
        "SentenceTransformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Probabilités",
        "Tests statistiques",
        "Régression statistique",
        "Optimisation mathématique"
      ],
      "bloc4_tools": [
        "Statsmodels",
        "SciPy",
        "SymPy",
        "R"
      ],
      "bloc5_likert": 5,
      "bloc5_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Apache Spark",
        "Kafka",
        "Hadoop",
        "Databricks"
      ],
      "bloc5_tools": [
        "GCP",
        "AWS",
        "Azure",
        "Docker"
      ],
      "bloc6_likert": 5,
      "bloc6_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Data storytelling",
        "Définition de KPIs",
        "Présentations business"
      ],
      "bloc6_tools": [
        "Tableau",
        "PowerBI",
        "Excel",
        "Google Slides"
      ],
      "bloc7_likert": 5,
      "bloc7_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "RGPD / Conformité",
        "Qualité des données",
        "Détection de biais IA",
        "Traçabilité"
      ],
      "bloc7_tools": [
        "RBAC",
        "Data Catalog",
        "Audit logs",
        "Outils d'anonymisation"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Requêtes complexes (JOIN, CTE)",
        "Window functions",
        "Modélisation de bases de données",
        "Optimisation de requêtes"
      ],
      "bloc8_tools": [
        "MySQL",
        "PostgreSQL",
        "MongoDB",
        "SQLite"
      ],
      "bloc9_likert": 5,
      "bloc9_text": "I performed data cleaning using Python and Pandas.\nI analyzed datasets and created visualizations with Matplotlib and Seaborn.\nI wrote SQL queries to extract and manipulate data from relational databases.\nI developed dashboards and reports in Power BI and Tableau.\nI conducted statistical analysis, including regression and hypothesis testing.\nI performed data wrangling to prepare datasets for modeling.\nI built interactive Excel reports and pivot tables for data exploration.\nI carried out data profiling and ensured data quality.\nI applied ETL basics to load and transform data efficiently.",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "CI/CD pour ML",
        "Déploiement de modèles",
        "Monitoring de modèles",
        "MLflow / DVC"
      ],
      "bloc9_tools": [
        "MLflow",
        "DVC",
        "Airflow",
        "FastAPI"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T13:48:51.040701Z",
    "responses": {
      "bloc1_likert": 2,
      "bloc1_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Matplotlib",
        "Pandas",
        "Plotly"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Classification",
        "Clustering",
        "Régression",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "XGBoost",
        "PyTorch",
        "scikit-learn",
        "TensorFlow"
      ],
      "bloc3_likert": 5,
      "bloc3_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Classification de texte",
        "Tokenization",
        "Embeddings",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "bloc3_tools": [
        "NLTK",
        "Transformers",
        "spaCy"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Probabilités",
        "Tests statistiques",
        "Régression statistique",
        "Optimisation mathématique"
      ],
      "bloc4_tools": [],
      "bloc5_likert": 5,
      "bloc5_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Hadoop",
        "Kafka",
        "Apache Spark",
        "Databricks"
      ],
      "bloc5_tools": [],
      "bloc6_likert": 5,
      "bloc6_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Définition de KPIs",
        "Création de dashboards",
        "Data storytelling",
        "Présentations business"
      ],
      "bloc6_tools": [
        "Google Slides",
        "Tableau",
        "Excel",
        "PowerBI"
      ],
      "bloc7_likert": 5,
      "bloc7_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "Qualité des données",
        "Traçabilité",
        "RGPD / Conformité",
        "Détection de biais IA"
      ],
      "bloc7_tools": [
        "Data Catalog",
        "RBAC",
        "Audit logs"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Requêtes complexes (JOIN, CTE)",
        "Window functions",
        "Modélisation de bases de données",
        "Optimisation de requêtes"
      ],
      "bloc8_tools": [
        "PostgreSQL",
        "MySQL",
        "SQLite",
        "MongoDB"
      ],
      "bloc9_likert": 5,
      "bloc9_text": "J'ai fait un projet d'analyse de données avec Python et Pandas",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "Déploiement de modèles",
        "Monitoring de modèles",
        "CI/CD pour ML",
        "MLflow / DVC"
      ],
      "bloc9_tools": [
        "MLflow",
        "DVC",
        "Airflow",
        "FastAPI"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T14:04:38.077201Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "J'ai réalisé une analyse exploratoire de données sur un dataset de ventes e-commerce avec Python et Pandas. J'ai nettoyé les données, créé des visualisations avec Matplotlib et Seaborn, et identifié les tendances clés pour optimiser la stratégie marketing.",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Matplotlib",
        "Pandas",
        "Plotly"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "J'ai développé un modèle de classification pour prédire le churn client en utilisant scikit-learn. J'ai testé plusieurs algorithmes (Random Forest, XGBoost) et optimisé les hyperparamètres avec GridSearch pour atteindre 85% de précision.",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Classification",
        "Clustering",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "XGBoost",
        "scikit-learn"
      ],
      "bloc3_likert": 5,
      "bloc3_text": "J'ai créé un système de classification de sentiments sur des avis clients en utilisant BERT et Transformers. Le modèle analyse les commentaires et détecte automatiquement les émotions positives, négatives ou neutres.",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Tokenization",
        "Classification de texte",
        "Embeddings",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "bloc3_tools": [
        "NLTK",
        "spaCy",
        "Transformers",
        "SentenceTransformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "J'ai réalisé des tests statistiques d'A/B testing pour comparer deux versions d'une interface web. J'ai utilisé des tests de Student et calculé les intervalles de confiance pour valider les résultats et prendre des décisions data-driven.",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Probabilités",
        "Tests statistiques",
        "Régression statistique",
        "Optimisation mathématique"
      ],
      "bloc4_tools": [
        "SciPy",
        "Statsmodels",
        "SymPy",
        "R"
      ],
      "bloc5_likert": 5,
      "bloc5_text": "J'ai travaillé sur AWS pour déployer un pipeline de traitement de données avec S3 et Lambda. J'ai utilisé Spark pour traiter de gros volumes de données et automatiser l'ingestion dans une base Redshift.",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Apache Spark",
        "Kafka",
        "Hadoop",
        "Databricks"
      ],
      "bloc5_tools": [
        "GCP",
        "Azure",
        "AWS",
        "Docker"
      ],
      "bloc6_likert": 5,
      "bloc6_text": "J'ai créé des dashboards interactifs avec PowerBI pour présenter les KPIs aux équipes business. J'ai traduit des insights techniques complexes en recommandations actionnables pour la direction.",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Data storytelling",
        "Définition de KPIs",
        "Présentations business"
      ],
      "bloc6_tools": [
        "Tableau",
        "Excel",
        "Google Slides",
        "PowerBI"
      ],
      "bloc7_likert": 5,
      "bloc7_text": "J'ai participé à la mise en conformité RGPD d'une base de données clients. J'ai implémenté des mécanismes d'anonymisation et documenté les processus de traitement des données personnelles.",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "Qualité des données",
        "Traçabilité",
        "Détection de biais IA",
        "RGPD / Conformité"
      ],
      "bloc7_tools": [
        "Audit logs",
        "RBAC",
        "Data Catalog"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "J'ai optimisé des requêtes SQL complexes avec des JOIN et des window functions pour améliorer les performances d'une base PostgreSQL. J'ai réduit le temps d'exécution de 50% en créant des index appropriés.",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Window functions",
        "Modélisation de bases de données",
        "Requêtes complexes (JOIN, CTE)"
      ],
      "bloc8_tools": [
        "SQLite"
      ],
      "bloc9_likert": 5,
      "bloc9_text": "J'ai mis en place un pipeline CI/CD pour déployer automatiquement des modèles ML en production avec Docker et MLflow. J'ai configuré le monitoring des performances et le retraining automatique des modèles.",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "Monitoring de modèles",
        "Déploiement de modèles",
        "CI/CD pour ML"
      ],
      "bloc9_tools": [
        "Airflow"
      ]
    }
  },
  {
    "user_id": 1,
    "user_name": "",
    "submitted_at": "2026-01-13T14:17:17.887725Z",
    "responses": {
      "bloc1_likert": 5,
      "bloc1_text": "J'ai réalisé une analyse exploratoire de données sur un dataset de ventes e-commerce avec Python et Pandas. J'ai nettoyé les données, créé des visualisations avec Matplotlib et Seaborn, et identifié les tendances clés pour optimiser la stratégie marketing.",
      "bloc1_yesno": "Oui",
      "bloc1_tasks": [
        "Nettoyage de données",
        "Analyse exploratoire (EDA)",
        "Visualisation",
        "Feature engineering"
      ],
      "bloc1_tools": [
        "NumPy",
        "Pandas",
        "Matplotlib",
        "Plotly"
      ],
      "bloc2_likert": 5,
      "bloc2_text": "J'ai développé un modèle de classification pour prédire le churn client en utilisant scikit-learn. J'ai testé plusieurs algorithmes (Random Forest, XGBoost) et optimisé les hyperparamètres avec GridSearch pour atteindre 85% de précision.",
      "bloc2_yesno": "Oui",
      "bloc2_tasks": [
        "Régression",
        "Classification",
        "Clustering",
        "Systèmes de recommandation"
      ],
      "bloc2_tools": [
        "TensorFlow",
        "scikit-learn",
        "XGBoost",
        "PyTorch"
      ],
      "bloc3_likert": 5,
      "bloc3_text": "J'ai créé un système de classification de sentiments sur des avis clients en utilisant BERT et Transformers. Le modèle analyse les commentaires et détecte automatiquement les émotions positives, négatives ou neutres.",
      "bloc3_yesno": "Oui",
      "bloc3_tasks": [
        "Tokenization",
        "Classification de texte",
        "Embeddings",
        "RAG (Retrieval-Augmented Generation)"
      ],
      "bloc3_tools": [
        "spaCy",
        "NLTK",
        "Transformers",
        "SentenceTransformers"
      ],
      "bloc4_likert": 5,
      "bloc4_text": "J'ai réalisé des tests statistiques d'A/B testing pour comparer deux versions d'une interface web. J'ai utilisé des tests de Student et calculé les intervalles de confiance pour valider les résultats et prendre des décisions data-driven.",
      "bloc4_yesno": "Oui",
      "bloc4_tasks": [
        "Optimisation mathématique",
        "Régression statistique",
        "Tests statistiques",
        "Probabilités"
      ],
      "bloc4_tools": [
        "SymPy",
        "R",
        "SciPy",
        "Statsmodels"
      ],
      "bloc5_likert": 5,
      "bloc5_text": "J'ai travaillé sur AWS pour déployer un pipeline de traitement de données avec S3 et Lambda. J'ai utilisé Spark pour traiter de gros volumes de données et automatiser l'ingestion dans une base Redshift.",
      "bloc5_yesno": "Oui",
      "bloc5_tasks": [
        "Apache Spark",
        "Kafka",
        "Hadoop",
        "Databricks"
      ],
      "bloc5_tools": [
        "GCP",
        "Azure",
        "AWS",
        "Docker"
      ],
      "bloc6_likert": 5,
      "bloc6_text": "J'ai créé des dashboards interactifs avec PowerBI pour présenter les KPIs aux équipes business. J'ai traduit des insights techniques complexes en recommandations actionnables pour la direction.",
      "bloc6_yesno": "Oui",
      "bloc6_tasks": [
        "Création de dashboards",
        "Data storytelling",
        "Définition de KPIs",
        "Présentations business"
      ],
      "bloc6_tools": [
        "Tableau",
        "Excel",
        "Google Slides",
        "PowerBI"
      ],
      "bloc7_likert": 5,
      "bloc7_text": "J'ai participé à la mise en conformité RGPD d'une base de données clients. J'ai implémenté des mécanismes d'anonymisation et documenté les processus de traitement des données personnelles.",
      "bloc7_yesno": "Oui",
      "bloc7_tasks": [
        "RGPD / Conformité",
        "Qualité des données",
        "Traçabilité",
        "Détection de biais IA"
      ],
      "bloc7_tools": [
        "RBAC",
        "Audit logs",
        "Data Catalog",
        "Outils d'anonymisation"
      ],
      "bloc8_likert": 5,
      "bloc8_text": "J'ai optimisé des requêtes SQL complexes avec des JOIN et des window functions pour améliorer les performances d'une base PostgreSQL. J'ai réduit le temps d'exécution de 50% en créant des index appropriés.",
      "bloc8_yesno": "Oui",
      "bloc8_tasks": [
        "Requêtes complexes (JOIN, CTE)",
        "Window functions",
        "Modélisation de bases de données",
        "Optimisation de requêtes"
      ],
      "bloc8_tools": [
        "MySQL",
        "PostgreSQL",
        "SQLite",
        "MongoDB"
      ],
      "bloc9_likert": 5,
      "bloc9_text": "J'ai mis en place un pipeline CI/CD pour déployer automatiquement des modèles ML en production avec Docker et MLflow. J'ai configuré le monitoring des performances et le retraining automatique des modèles.",
      "bloc9_yesno": "Oui",
      "bloc9_tasks": [
        "Monitoring de modèles",
        "Déploiement de modèles",
        "CI/CD pour ML",
        "MLflow / DVC"
      ],
      "bloc9_tools": [
        "DVC",
        "MLflow",
        "Airflow",
        "FastAPI"
      ]
    }
  }
]