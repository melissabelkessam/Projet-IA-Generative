"""
Moteur d'analyse s√©mantique SBERT
√âtape 3 : Correspondance s√©mantique (Semantic Matching)
√âtape 4 : Calcul du Coverage Score (Score de Couverture Global)
√âtape 5 : Recommandation des 3 meilleurs m√©tiers
Version am√©lior√©e : Utilise textes libres + t√¢ches + outils
"""

import pandas as pd
import json
import os
from sentence_transformers import SentenceTransformer, util
import numpy as np

# =========================
# Configuration
# =========================
COMPETENCIES_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "competencies.csv")
JOBS_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "jobs.csv")
USER_RESPONSES_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "user_responses.json")
SBERT_MODEL = "all-MiniLM-L6-v2"

# =========================
# Fonction 1 : Charger et concat√©ner les comp√©tences par bloc
# =========================
def load_and_concatenate_blocks():
    """
    Charge le r√©f√©rentiel de comp√©tences et concat√®ne toutes les comp√©tences
    de chaque bloc en une seule grande cha√Æne de texte.
    
    Retourne un dictionnaire : {BlockID: texte_concat√©n√©}
    """
    print("üìÇ Chargement du r√©f√©rentiel de comp√©tences...")
    df = pd.read_csv(COMPETENCIES_PATH)
    
    blocks_text = {}
    
    # Regrouper par BlockID
    for block_id in sorted(df['BlockID'].unique()):
        # Prendre toutes les comp√©tences du bloc
        competencies = df[df['BlockID'] == block_id]['Competency'].tolist()
        
        # Concat√©ner toutes les comp√©tences en une seule phrase
        blocks_text[block_id] = ' '.join(competencies)
        
        block_name = df[df['BlockID'] == block_id]['BlockName'].iloc[0]
        print(f"   ‚úÖ Bloc {block_id} ({block_name}) : {len(competencies)} comp√©tences")
    
    return blocks_text

# =========================
# Fonction 2 : Charger les r√©ponses utilisateur (VERSION AM√âLIOR√âE)
# =========================
def load_user_responses():
    """
    Charge les r√©ponses utilisateur depuis le JSON.
    Extrait :
    - Les textes libres (_text)
    - Les t√¢ches ma√Ætris√©es (_tasks)
    - Les outils ma√Ætris√©s (_tools)
    
    Retourne un texte enrichi combinant toutes ces informations.
    """
    print("\nüìÇ Chargement des r√©ponses utilisateur...")
    
    with open(USER_RESPONSES_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Prendre la derni√®re r√©ponse soumise
    if isinstance(data, list):
        user_data = data[-1]  # Derni√®re soumission
    else:
        user_data = data
    
    responses = user_data.get('responses', {})
    
    # Listes pour stocker les diff√©rents types de r√©ponses
    user_texts = []
    user_tasks = []
    user_tools = []
    
    # Parcourir toutes les r√©ponses
    for key, value in responses.items():
        
        # 1. Textes libres
        if key.endswith('_text') and isinstance(value, str) and value.strip() != "":
            user_texts.append(value)
            print(f"   ‚úÖ Texte trouv√© : {value[:50]}...")
        
        # 2. T√¢ches (listes)
        elif key.endswith('_tasks') and isinstance(value, list):
            # Filtrer "Aucune de ces t√¢ches"
            filtered_tasks = [task for task in value if task != "Aucune de ces t√¢ches"]
            if filtered_tasks:
                user_tasks.extend(filtered_tasks)
                print(f"   üéØ T√¢ches : {', '.join(filtered_tasks)}")
        
        # 3. Outils (listes)
        elif key.endswith('_tools') and isinstance(value, list):
            if value:
                user_tools.extend(value)
                print(f"   üõ†Ô∏è Outils : {', '.join(value)}")
    
    # Combiner tout en un seul texte enrichi
    combined_parts = []
    
    if user_texts:
        combined_parts.append(' '.join(user_texts))
    
    if user_tasks:
        combined_parts.append(' '.join(user_tasks))
    
    if user_tools:
        combined_parts.append(' '.join(user_tools))
    
    combined_user_text = ' '.join(combined_parts)
    
    print(f"\nüìù Texte enrichi total : {len(combined_user_text)} caract√®res")
    print(f"   - {len(user_texts)} textes libres")
    print(f"   - {len(user_tasks)} t√¢ches")
    print(f"   - {len(user_tools)} outils")
    
    return combined_user_text

# =========================
# Fonction 3 : Calculer la similarit√© s√©mantique
# =========================
def calculate_semantic_similarity(user_text, blocks_text):
    """
    Encode le texte utilisateur et les textes des blocs avec SBERT.
    Calcule la similarit√© cosinus entre l'utilisateur et chaque bloc.
    
    Retourne un dictionnaire : {BlockID: score_similarit√©}
    """
    print("\nü§ñ Chargement du mod√®le SBERT...")
    model = SentenceTransformer(SBERT_MODEL)
    
    print("üîÑ Encodage du texte utilisateur enrichi...")
    user_embedding = model.encode(user_text, convert_to_tensor=True)
    
    print("üîÑ Encodage des blocs de comp√©tences...")
    block_scores = {}
    
    for block_id, block_text in blocks_text.items():
        # Encoder le bloc
        block_embedding = model.encode(block_text, convert_to_tensor=True)
        
        # Calculer la similarit√© cosinus
        similarity = util.cos_sim(user_embedding, block_embedding)
        score = float(similarity[0][0])
        
        block_scores[block_id] = round(score, 4)
        print(f"   üìä Bloc {block_id} : {score:.4f}")
    
    return block_scores

# =========================
# Fonction 4 : Calculer le Coverage Score
# =========================
def calculate_coverage_score(block_scores, weights=None):
    """
    Calcule le Coverage Score global selon la formule de la prof.
    """
    
    # Si pas de poids fournis, tous les blocs ont un poids de 1
    if weights is None:
        weights = {block_id: 1.0 for block_id in block_scores.keys()}
    
    # Calcul du num√©rateur : somme des (poids √ó score)
    weighted_sum = sum(weights[block_id] * score for block_id, score in block_scores.items())
    
    # Calcul du d√©nominateur : somme des poids
    total_weight = sum(weights.values())
    
    # Coverage Score
    coverage_score = weighted_sum / total_weight if total_weight > 0 else 0.0
    
    return round(coverage_score, 4)

# =========================
# Fonction 5 : Mapper les comp√©tences aux blocs (NOUVEAU - √âTAPE 5)
# =========================
def create_competency_to_block_mapping():
    """
    Cr√©e un dictionnaire qui mappe chaque CompetencyID √† son BlockID.
    
    Retourne : {CompetencyID: BlockID}
    Exemple : {'C001': 1, 'C002': 1, 'C101': 2, ...}
    """
    df = pd.read_csv(COMPETENCIES_PATH)
    
    mapping = {}
    for _, row in df.iterrows():
        mapping[row['CompetencyID']] = row['BlockID']
    
    return mapping

# =========================
# Fonction 6 : Calculer le score d'un m√©tier (NOUVEAU - √âTAPE 5)
# =========================
def calculate_job_score(required_competencies, block_scores, comp_to_block_mapping):
    """
    Calcule le score d'un m√©tier bas√© sur ses comp√©tences requises.
    
    Logique :
    - Pour chaque comp√©tence requise, trouve son bloc
    - Prend le score du bloc correspondant
    - Fait la moyenne de tous les scores
    
    Args:
        required_competencies (list): Liste des CompetencyID requis (ex: ['C001', 'C002', 'C101'])
        block_scores (dict): Scores par bloc {BlockID: score}
        comp_to_block_mapping (dict): Mapping {CompetencyID: BlockID}
    
    Returns:
        float: Score du m√©tier (moyenne des scores des blocs concern√©s)
    """
    
    scores = []
    
    for comp_id in required_competencies:
        # Trouver le bloc de cette comp√©tence
        block_id = comp_to_block_mapping.get(comp_id)
        
        if block_id and block_id in block_scores:
            scores.append(block_scores[block_id])
    
    # Calculer la moyenne
    if scores:
        return round(sum(scores) / len(scores), 4)
    else:
        return 0.0

# =========================
# Fonction 7 : Recommander les m√©tiers (NOUVEAU - √âTAPE 5)
# =========================
def recommend_jobs(block_scores):
    """
    Recommande les 3 meilleurs m√©tiers bas√©s sur les scores de blocs.
    
    Args:
        block_scores (dict): Scores par bloc
    
    Returns:
        list: Top 3 m√©tiers avec leurs scores
              Format : [{'job_id': 'J01', 'title': '...', 'score': 0.85, 'description': '...'}, ...]
    """
    print("\n" + "=" * 60)
    print("üéØ RECOMMANDATION DE M√âTIERS - √âTAPE 5")
    print("=" * 60)
    
    # Charger les m√©tiers
    print("\nüìÇ Chargement des m√©tiers...")
    jobs_df = pd.read_csv(JOBS_PATH)
    
    # Cr√©er le mapping comp√©tence ‚Üí bloc
    comp_to_block = create_competency_to_block_mapping()
    
    # Calculer le score de chaque m√©tier
    job_scores = []
    
    for _, job in jobs_df.iterrows():
        # Extraire les comp√©tences requises (format: "C001;C002;C011")
        required_comps = job['RequiredCompetencies'].split(';')
        
        # Calculer le score du m√©tier
        job_score = calculate_job_score(required_comps, block_scores, comp_to_block)
        
        job_scores.append({
            'job_id': job['JobID'],
            'title': job['JobTitle'],
            'score': job_score,
            'description': job['Description']
        })
        
        print(f"   üìä {job['JobTitle']} : {job_score:.4f}")
    
    # Trier par score d√©croissant et prendre les 3 premiers
    top_3_jobs = sorted(job_scores, key=lambda x: x['score'], reverse=True)[:3]
    
    print("\n" + "=" * 60)
    print("‚úÖ TOP 3 M√âTIERS RECOMMAND√âS :")
    print("=" * 60)
    for i, job in enumerate(top_3_jobs, 1):
        print(f"{i}. {job['title']} - Score: {job['score']:.4f} ({job['score']*100:.1f}%)")
    
    return top_3_jobs

# =========================
# Fonction principale : Analyse compl√®te (MODIFI√âE)
# =========================
def analyze_user_profile():
    """
    Fonction principale qui orchestre toute l'analyse s√©mantique.
    
    Retourne un dictionnaire contenant :
    - 'block_scores': scores par bloc
    - 'coverage_score': score de couverture global
    - 'recommended_jobs': top 3 m√©tiers recommand√©s
    """
    print("=" * 60)
    print("üéØ ANALYSE S√âMANTIQUE COMPL√àTE - √âTAPES 3, 4 & 5")
    print("=" * 60)
    
    # 1. Charger et concat√©ner les comp√©tences par bloc
    blocks_text = load_and_concatenate_blocks()
    
    # 2. Charger les r√©ponses utilisateur (textes + t√¢ches + outils)
    user_text = load_user_responses()
    
    if not user_text.strip():
        print("\n‚ùå ERREUR : Aucune information trouv√©e dans les r√©ponses utilisateur !")
        return None
    
    # 3. Calculer la similarit√© s√©mantique (√âtape 3)
    block_scores = calculate_semantic_similarity(user_text, blocks_text)
    
    # 4. Calculer le Coverage Score (√âtape 4)
    coverage_score = calculate_coverage_score(block_scores)
    
    print("\n" + "=" * 60)
    print(f"üéØ COVERAGE SCORE GLOBAL : {coverage_score} ({coverage_score*100:.1f}%)")
    print("=" * 60)
    
    # 5. Recommander les m√©tiers (√âtape 5)
    recommended_jobs = recommend_jobs(block_scores)
    
    print("\n" + "=" * 60)
    print("‚úÖ ANALYSE COMPL√àTE TERMIN√âE")
    print("=" * 60)
    
    # Retourner les r√©sultats
    return {
        'block_scores': block_scores,
        'coverage_score': coverage_score,
        'recommended_jobs': recommended_jobs
    }

# =========================
# Test du module
# =========================
if __name__ == "__main__":
    results = analyze_user_profile()
    
    if results:
        print("\nüìä R√âSULTATS FINAUX :")
        print("-" * 40)
        print(f"Coverage Score : {results['coverage_score']} ({results['coverage_score']*100:.1f}%)")
        print("-" * 40)
        print("\nüèÜ TOP 3 M√âTIERS :")
        for i, job in enumerate(results['recommended_jobs'], 1):
            print(f"{i}. {job['title']} - {job['score']*100:.1f}%")